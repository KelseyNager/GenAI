{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KelseyNager/GenAI/blob/main/Problem_1.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSrjyb8l1uzT"
      },
      "source": [
        "#Text Generation with LSTM\n",
        "##Kelsey Nager\n",
        "##CSC 330\n",
        "\n",
        "The purpose of this assignment is to train a LSTM model on three of Virginia Woolf's books and generate text in a similar language. I will input \"Mrs. Dalloway\", \"Common Reader\", and \"The Voyage Out\" by Virginia Woolf from *Project Gutenberg* online platfom. I will create a single and multi-layer LSTM and experiment with parameters to discover the most effective model for generating coherent text comparable to the style of Woolf."
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Parameters"
      ],
      "metadata": {
        "id": "3nzw9tOeWyWt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P2rzFk5-rblV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U3egQ336rwa_"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 20000 #accomodates a vocabulary size of 18612\n",
        "MAX_LEN = 150\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ9DJyoq1wUJ"
      },
      "source": [
        "#Data Collection and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfqXZdFdp4C7",
        "outputId": "9ac75819-b2cc-4f58-bf1d-382b9b168912"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start match found: True\n",
            "End match found: True\n",
            "Start match found: True\n",
            "End match found: True\n",
            "Start match found: True\n",
            "End match found: True\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import re\n",
        "\n",
        "#Trimming the book content so that unnecessary commentary on the site is excluded from training data\n",
        "def trim_book_content(book_content, start, end):\n",
        "    \"\"\"Trims the beginning and end of book content using markers.\"\"\"\n",
        "    start_match = re.search(re.escape(start), book_content)\n",
        "    end_match = re.search(re.escape(end), book_content)\n",
        "\n",
        "    print(f\"Start match found: {start_match is not None}\")  # Check if start marker is found\n",
        "    print(f\"End match found: {end_match is not None}\")    # Check if end marker is found\n",
        "\n",
        "    if start_match and end_match:\n",
        "        start_index = start_match.end()\n",
        "        end_index = end_match.start()\n",
        "        trimmed_content = book_content[start_index:end_index]\n",
        "        return trimmed_content\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "# Download each text file and append to all_books\n",
        "urls = [\n",
        "\"https://www.gutenberg.org/files/71865/71865-0.txt\",  # Mrs Dalloway, Virginia Woolf\n",
        "\"https://www.gutenberg.org/files/144/144-0.txt\",   # The Voyage Out, Virginia Woolf\n",
        "\"https://www.gutenberg.org/files/64457/64457-0.txt\"   # The Common Reader, Virginia Woolf\n",
        "      ]\n",
        "\n",
        "start = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "end = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "all_books = \"\"\n",
        "\n",
        "# Save combined trimmed text to a single file\n",
        "for url in urls:\n",
        "  response = requests.get(url)\n",
        "  book_content = response.text\n",
        "  trimmed_text = trim_book_content(book_content, start, end)\n",
        "  all_books += trimmed_text + \"\\n\\n\"\n",
        "\n",
        "with open('all_books_trimmed.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(all_books)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"all_books_trimmed.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    all_books = file.read()\n",
        "\n",
        "# Split the text into lines\n",
        "book_data = all_books.split(\"\\n\")\n",
        "\n",
        "#Filtered_data represents all three combined, filtered Vrignia Woolf books splint into lines\n",
        "filtered_data = [\n",
        "    \"Text: \" + line\n",
        "    for line in book_data\n",
        "    if line.strip()\n",
        "]"
      ],
      "metadata": {
        "id": "p-qRWzQzPpmX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KAvTF1SsK4eY",
        "outputId": "d067eaa7-f55a-4911-d3fb-a4b2ddb495a2"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Text: Elizabeth), and she, too, loving it as she did with an absurd and'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 33
        }
      ],
      "source": [
        "# Display an example word\n",
        "example = filtered_data[100]\n",
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "70pfkORxsJYh"
      },
      "outputs": [],
      "source": [
        "def pad_punctuation(s):\n",
        "    s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s)  # Pad punctuation\n",
        "    s = re.sub(\" +\", \" \", s)\n",
        "    s = s.lower()  # Convert to lowercase for consistency\n",
        "    s = s.replace(\"\\xe2\\x80\\x90\", \"'\") #replace encoding with apostrophe\n",
        "    s = s.replace(\"\\xe2\\x80\\x94\", \"—\") # replace with dash\n",
        "    s = s.replace(\"\\xe2\\x80\\x9d\", '\"') # Replace with right double quote\n",
        "    s = s.replace(\"\\xe2\\x80\\x9c\", '\"') # Replace with left double quote\n",
        "    return s\n",
        "\n",
        "text_data = [pad_punctuation(s) for s in filtered_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-UOtajoL6oi",
        "outputId": "8894f461-4338-4331-8406-56061a7d8094"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines of text of filtered data: 24761\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of lines of text of filtered data: {len(filtered_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#same example as earlier, now with padded punctuation and lowercase letters\n",
        "example_data = text_data[100]\n",
        "example_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aeJ8tDHvRvWt",
        "outputId": "501311ca-daa3-43d5-bca0-60e8a31fa2f4"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'text : elizabeth ) , and she , too , loving it as she did with an absurd and'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 36
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "mr9FuN79sKT6"
      },
      "outputs": [],
      "source": [
        "# Convert to a Tensorflow Dataset\n",
        "text_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(text_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(1000)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#example of lines\n",
        "for example in text_ds.take(1):\n",
        "       print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu-F6VYiEcWw",
        "outputId": "dd0efa21-8845-48d4-ca97-941ebda89d41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'text : cliff . we know no more of them than that . we have their poetry , and that'\n",
            " b'text : is all . '\n",
            " b'text : but that is not , and perhaps never can be , wholly true . pick up any play'\n",
            " b'text : by sophocles , read\\xe2\\x80\\x94'\n",
            " b'text : son of him who led our hosts at troy of old , son of'\n",
            " b'text : agamemnon , '\n",
            " b'text : and at once the mind begins to fashion itself surroundings . it makes'\n",
            " b'text : some background , even of the most provisional sort , for sophocles ; it'\n",
            " b'text : imagines some village , in a remote part of the country , near the sea . '\n",
            " b'text : even nowadays such villages are to be found in the wilder parts of'\n",
            " b'text : england , and as we enter them we can scarcely help feeling that here , in'\n",
            " b'text : this cluster of cottages , cut off from rail or city , are all the'\n",
            " b'text : elements of a perfect existence . here is the rectory ; here the manor'\n",
            " b'text : house , the farm and the cottages ; the church for worship , the club for'\n",
            " b'text : meeting , the cricket field for play . here life is simply sorted out into'\n",
            " b'text : its main elements . each man and woman has his work ; each works for the'\n",
            " b'text : health or happiness of others . and here , in this little community , '\n",
            " b'text : characters become part of the common stock ; the eccentricities of the'\n",
            " b'text : clergyman are known ; the great ladies\\xe2\\x80\\x99 defects of temper ; the'\n",
            " b'text : blacksmith\\xe2\\x80\\x99s feud with the milkman , and the loves and matings of the'\n",
            " b'text : boys and girls . here life has cut the same grooves for centuries ; '\n",
            " b'text : customs have arisen ; legends have attached themselves to hilltops and'\n",
            " b'text : solitary trees , and the village has its history , its festivals , and its'\n",
            " b'text : rivalries . '\n",
            " b'text : it is the climate that is impossible . if we try to think of sophocles'\n",
            " b'text : here , we must annihilate the smoke and the damp and the thick wet mists . '\n",
            " b'text : we must sharpen the lines of the hills . we must imagine a beauty of'\n",
            " b'text : stone and earth rather than of woods and greenery . with warmth and'\n",
            " b'text : sunshine and months of brilliant , fine weather , life of course is'\n",
            " b'text : instantly changed ; it is transacted out of doors , with the result , known'\n",
            " b'text : to all who visit italy , that small incidents are debated in the street , '\n",
            " b'text : not in the sitting - room , and become dramatic ; make people voluble ; '\n",
            " b'text : inspire in them that sneering , laughing , nimbleness of wit and tongue'\n",
            " b'text : peculiar to the southern races , which has nothing in common with the'\n",
            " b'text : slow reserve , the low half - tones , the brooding introspective melancholy'\n",
            " b'text : of people accustomed to live more than half the year indoors . '\n",
            " b'text : that is the quality that first strikes us in greek literature , the'\n",
            " b'text : lightning - quick , sneering , out - of - doors manner . it is apparent in the'\n",
            " b'text : most august as well as in the most trivial places . queens and princesses'\n",
            " b'text : in this very tragedy by sophocles stand at the door bandying words like'\n",
            " b'text : village women , with a tendency , as one might expect , to rejoice in'\n",
            " b'text : language , to split phrases into slices , to be intent on verbal victory . '\n",
            " b'text : the humour of the people was not good natured like that of our postmen'\n",
            " b'text : and cabdrivers . the taunts of men lounging at the street corners had'\n",
            " b'text : something cruel in them as well as witty . there is a cruelty in greek'\n",
            " b'text : tragedy which is quite unlike our english brutality : is not pentheus , '\n",
            " b'text : for example , that highly respectable man , made ridiculous in the'\n",
            " b'text : _ bacch\\xc3\\xa6 _ before he is destroyed ? in fact , of course , these queens and'\n",
            " b'text : princesses were out of doors , with the bees buzzing past them , shadows'\n",
            " b'text : crossing them , and the wind taking their draperies . they were speaking'\n",
            " b'text : to an enormous audience rayed round them on one of those brilliant'\n",
            " b'text : southern days when the sun is so hot and yet the air so exciting . the'\n",
            " b'text : poet , therefore , had to bethink him , not of some theme which could be'\n",
            " b'text : read for hours by people in privacy , but of something emphatic , '\n",
            " b'text : familiar , brief , that would carry , instantly and directly , to an'\n",
            " b'text : audience of seventeen thousand people , perhaps , with ears and eyes eager'\n",
            " b'text : and attentive , with bodies whose muscles would grow stiff if they sat'\n",
            " b'text : too long without diversion . music and dancing he would need , and'\n",
            " b'text : naturally would choose one of those legends , like our tristram and'\n",
            " b'text : iseult , which are known to every one in outline , so that a great fund of'\n",
            " b'text : emotion is ready prepared , but can be stressed in a new place by each'\n",
            " b'text : new poet . '\n",
            " b'text : sophocles would take the old story of electra , for instance , but would'\n",
            " b'text : at once impose his stamp upon it . of that , in spite of our weakness and'], shape=(64,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZwRkI5HbsKXb"
      },
      "outputs": [],
      "source": [
        "# Create a vectorization layer\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower\",\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LEN + 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7jgxCJ6XsR30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8ea60581-a548-40b9-d474-26fef426e4dd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 18612\n"
          ]
        }
      ],
      "source": [
        "# Adapt the layer to the training set\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "print(\"Vocabulary size:\", len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1uQdQAa65ga",
        "outputId": "8af3d977-4104-41be-9bd5-b13b25a620cf"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \n",
            "1: [UNK]\n",
            "2: :\n",
            "3: text\n",
            "4: ,\n",
            "5: the\n",
            "6: .\n",
            "7: and\n",
            "8: of\n",
            "9: to\n"
          ]
        }
      ],
      "source": [
        "# Display some token:word mappings\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "    print(f\"{i}: {word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sfgAHOdp118j"
      },
      "outputs": [],
      "source": [
        "# Create the training set of book content and the same text shifted by one word\n",
        "def prepare_inputs(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    return x, tokenized_sentences[:, 1:]\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZsUQObW2pgu"
      },
      "source": [
        "# Single-Layer LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "MI4ym9622psy",
        "outputId": "f15cf3c7-d149-44ae-f364-8dd9b5a25f8c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │       \u001b[38;5;34m2,500,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m117,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25000\u001b[0m)         │       \u001b[38;5;34m3,225,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,500,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25000</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,225,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,842,248\u001b[0m (22.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,842,248</span> (22.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,842,248\u001b[0m (22.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,842,248</span> (22.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "#Creating a single-layer LSTM model with dropout = .2\n",
        "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x = layers.LSTM(128, return_sequences=True, dropout=0.2)(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "lstm_1 = models.Model(inputs, outputs)\n",
        "lstm_1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61bA3bh6216j"
      },
      "source": [
        "#Training Single-Layer LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "WBzbSddk2zNa"
      },
      "outputs": [],
      "source": [
        "loss_fn = losses.SparseCategoricalCrossentropy()\n",
        "lstm_1.compile(\"adam\", loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YbRUW4D03L9j"
      },
      "outputs": [],
      "source": [
        "# Create a TextGenerator checkpoint\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index\n",
        "            for index, word in enumerate(index_to_word)\n",
        "        }\n",
        "\n",
        "    def sample_from(self, probs, temperature):\n",
        "        if isinstance(probs, (float, np.float64)):  # Check if probs is a single value\n",
        "            probs = np.array([probs, 1 - probs])  # Create a 2-element distribution\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len([\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]) < max_tokens and sample_token != 0:\n",
        "            y = self.model.predict(np.array([[\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]]))\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
        "\n",
        "            if 0 <= sample_token < len(self.index_to_word):  # Check if sample_token is within range\n",
        "              start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "              info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "              [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ].append(sample_token)\n",
        "            else:\n",
        "              # Handle case where sample_token is out of range\n",
        "              print(f\"Warning: sample_token out of range: {sample_token}\")\n",
        "              break\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "            info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "            [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ].append(sample_token)\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      try:\n",
        "        prompts = ('the meaning of life', 'it is an awful')\n",
        "        prompt = np.random.choice(prompts)\n",
        "        self.generate(prompt, max_tokens=100, temperature=.5)\n",
        "      except Exception as e:\n",
        "        print(f\"Error during text generation: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dMBBD8kx2_yr"
      },
      "outputs": [],
      "source": [
        "# Tokenize starting prompt\n",
        "\n",
        "text_generator = TextGenerator(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ-tfbj23C1z",
        "outputId": "f72536a6-e0fc-48b7-dd5b-fa9d4c2f3f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Warning: sample_token out of range: 18667\n",
            "\n",
            "generated text:\n",
            "it is an awful intellectuals intellectuals jingle jingle\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 150ms/step - loss: 0.5625\n",
            "Epoch 2/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life vinraces vinraces  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 150ms/step - loss: 0.5470\n",
            "Epoch 3/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Warning: sample_token out of range: 21866\n",
            "\n",
            "generated text:\n",
            "it is an awful\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 151ms/step - loss: 0.5354\n",
            "Epoch 4/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 23426\n",
            "\n",
            "generated text:\n",
            "it is an awful sniffing sniffing\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 151ms/step - loss: 0.5187\n",
            "Epoch 5/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Warning: sample_token out of range: 20871\n",
            "\n",
            "generated text:\n",
            "it is an awful olney olney\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 151ms/step - loss: 0.5102\n",
            "Epoch 6/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 21154\n",
            "\n",
            "generated text:\n",
            "it is an awful\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 151ms/step - loss: 0.4984\n",
            "Epoch 7/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life stood stood . .  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 152ms/step - loss: 0.4912\n",
            "Epoch 8/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life imbed imbed clutching clutching  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 153ms/step - loss: 0.4842\n",
            "Epoch 9/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Warning: sample_token out of range: 18924\n",
            "\n",
            "generated text:\n",
            "it is an awful unlikely unlikely\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 151ms/step - loss: 0.4823\n",
            "Epoch 10/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life charles charles was was  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 153ms/step - loss: 0.4745\n",
            "Epoch 11/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Warning: sample_token out of range: 23665\n",
            "\n",
            "generated text:\n",
            "the meaning of life\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 153ms/step - loss: 0.4677\n",
            "Epoch 12/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life preserved preserved and and those those  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 153ms/step - loss: 0.4609\n",
            "Epoch 13/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life spain spain  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 151ms/step - loss: 0.4549\n",
            "Epoch 14/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 20593\n",
            "\n",
            "generated text:\n",
            "it is an awful\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 154ms/step - loss: 0.4529\n",
            "Epoch 15/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 19364\n",
            "\n",
            "generated text:\n",
            "it is an awful\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 152ms/step - loss: 0.4472\n",
            "Epoch 16/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 19283\n",
            "\n",
            "generated text:\n",
            "the meaning of life\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 152ms/step - loss: 0.4406\n",
            "Epoch 17/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Warning: sample_token out of range: 24584\n",
            "\n",
            "generated text:\n",
            "the meaning of life\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 154ms/step - loss: 0.4379\n",
            "Epoch 18/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Warning: sample_token out of range: 24495\n",
            "\n",
            "generated text:\n",
            "it is an awful\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 151ms/step - loss: 0.4312\n",
            "Epoch 19/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Warning: sample_token out of range: 19858\n",
            "\n",
            "generated text:\n",
            "it is an awful sharp sharp euripides euripides territory territory bencher bencher yet—i yet—i sized sized survive survive\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 154ms/step - loss: 0.4244\n",
            "Epoch 20/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 19235\n",
            "\n",
            "generated text:\n",
            "it is an awful par par affirmed affirmed women—her women—her terence’s terence’s attaching attaching books” books” famished famished ambrose—i ambrose—i\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 155ms/step - loss: 0.4223\n",
            "Epoch 21/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "Warning: sample_token out of range: 19458\n",
            "\n",
            "generated text:\n",
            "it is an awful angel angel\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 151ms/step - loss: 0.4187\n",
            "Epoch 22/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life pass pass create create . .  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 153ms/step - loss: 0.4121\n",
            "Epoch 23/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Warning: sample_token out of range: 20338\n",
            "\n",
            "generated text:\n",
            "the meaning of life of of worsted worsted fresshe fresshe\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 152ms/step - loss: 0.4090\n",
            "Epoch 24/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life crocodile crocodile  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 152ms/step - loss: 0.4065\n",
            "Epoch 25/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life scots scots dominoes dominoes ‘quite ‘quite antiquated antiquated  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 152ms/step - loss: 0.4044\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f0b896717e0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "lstm_1.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[text_generator],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go6WtJ5I3mzu"
      },
      "source": [
        "#Text Generation\n",
        "##with Single Layer LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ohzdYaVN38bq"
      },
      "outputs": [],
      "source": [
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            if 0 <= i < len(vocab):\n",
        "                print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "            else:\n",
        "                print(f\"Index {i} out of range for vocabulary (size: {len(vocab)})\") # Print error message\n",
        "        print(\"--------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prompt 1, Various Temperatures"
      ],
      "metadata": {
        "id": "BL6tVa0uUL7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SfN-DQI13ri6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236c13f2-5d8c-4d1f-ee35-3bf3840466c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life is tretys tretys , , and and\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is tretys\n",
            "laced:   \t0.48%\n",
            "famously:   \t0.46%\n",
            "cakes:   \t0.46%\n",
            "11:   \t0.43%\n",
            "voyage—china:   \t0.42%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is tretys tretys\n",
            "laced:   \t0.48%\n",
            "famously:   \t0.46%\n",
            "cakes:   \t0.46%\n",
            "11:   \t0.43%\n",
            "voyage—china:   \t0.42%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is tretys tretys ,\n",
            ",:   \t85.91%\n",
            ";:   \t11.17%\n",
            ":   \t1.95%\n",
            ".:   \t0.69%\n",
            "?:   \t0.19%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is tretys tretys , ,\n",
            ",:   \t85.91%\n",
            ";:   \t11.17%\n",
            ":   \t1.95%\n",
            ".:   \t0.69%\n",
            "?:   \t0.19%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is tretys tretys , , and\n",
            "and:   \t69.83%\n",
            "which:   \t21.35%\n",
            "”:   \t3.27%\n",
            "or:   \t1.68%\n",
            "but:   \t1.25%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is tretys tretys , , and and\n",
            "and:   \t69.83%\n",
            "which:   \t21.35%\n",
            "”:   \t3.27%\n",
            "or:   \t1.68%\n",
            "but:   \t1.25%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"the meaning of life is\", max_tokens=10, temperature=.2\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MnMJHTdu3viT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04965672-ac0a-4b8f-c49f-db54e21a6855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Warning: sample_token out of range: 23156\n",
            "\n",
            "generated text:\n",
            "the meaning of life is true true rated rated\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is true\n",
            "_:   \t2.75%\n",
            "a:   \t0.74%\n",
            "the:   \t0.52%\n",
            "that:   \t0.44%\n",
            "like:   \t0.4%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is true true\n",
            "_:   \t2.75%\n",
            "a:   \t0.74%\n",
            "the:   \t0.52%\n",
            "that:   \t0.44%\n",
            "like:   \t0.4%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is true true rated\n",
            "footnote:   \t0.66%\n",
            "8:   \t0.35%\n",
            "1:   \t0.32%\n",
            "7:   \t0.31%\n",
            "6:   \t0.3%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is true true rated rated\n",
            "footnote:   \t0.66%\n",
            "8:   \t0.35%\n",
            "1:   \t0.32%\n",
            "7:   \t0.31%\n",
            "6:   \t0.3%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"the meaning of life is\", max_tokens=10, temperature=0.5\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hPaf5l2n2ME1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b079704c-f024-4a4c-ca4b-768505b17f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire straighten straighten speedily speedily  \n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave\n",
            "_:   \t0.06%\n",
            "captured:   \t0.05%\n",
            "published:   \t0.04%\n",
            "calming:   \t0.04%\n",
            "4:   \t0.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave\n",
            "_:   \t0.06%\n",
            "captured:   \t0.05%\n",
            "published:   \t0.04%\n",
            "calming:   \t0.04%\n",
            "4:   \t0.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable\n",
            "a:   \t13.79%\n",
            "the:   \t10.29%\n",
            "my:   \t5.02%\n",
            "love:   \t4.59%\n",
            "an:   \t3.57%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable\n",
            "a:   \t13.79%\n",
            "the:   \t10.29%\n",
            "my:   \t5.02%\n",
            "love:   \t4.59%\n",
            "an:   \t3.57%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered\n",
            "footnote:   \t0.09%\n",
            "n’avoir:   \t0.05%\n",
            "delineator:   \t0.05%\n",
            "arm’s:   \t0.04%\n",
            "embark:   \t0.04%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered\n",
            "footnote:   \t0.09%\n",
            "n’avoir:   \t0.05%\n",
            "delineator:   \t0.05%\n",
            "arm’s:   \t0.04%\n",
            "embark:   \t0.04%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably\n",
            "footnote:   \t0.29%\n",
            "francis:   \t0.19%\n",
            "undergraduate:   \t0.15%\n",
            "harry:   \t0.12%\n",
            "atheist:   \t0.11%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably\n",
            "footnote:   \t0.29%\n",
            "francis:   \t0.19%\n",
            "undergraduate:   \t0.15%\n",
            "harry:   \t0.12%\n",
            "atheist:   \t0.11%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire\n",
            "morrow:   \t0.1%\n",
            "back—or:   \t0.05%\n",
            "and—do:   \t0.05%\n",
            "sadly:   \t0.05%\n",
            "on—as:   \t0.05%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire\n",
            "morrow:   \t0.1%\n",
            "back—or:   \t0.05%\n",
            "and—do:   \t0.05%\n",
            "sadly:   \t0.05%\n",
            "on—as:   \t0.05%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire straighten\n",
            "morrow:   \t0.18%\n",
            "footnote:   \t0.13%\n",
            "illustration:   \t0.05%\n",
            "breeches:   \t0.04%\n",
            "8:   \t0.04%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire straighten straighten\n",
            "morrow:   \t0.18%\n",
            "footnote:   \t0.13%\n",
            "illustration:   \t0.05%\n",
            "breeches:   \t0.04%\n",
            "8:   \t0.04%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire straighten straighten speedily\n",
            "morrow:   \t0.11%\n",
            "fasten:   \t0.06%\n",
            "euphrosyne:   \t0.06%\n",
            "inferno:   \t0.05%\n",
            "tatler:   \t0.05%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire straighten straighten speedily speedily\n",
            "morrow:   \t0.11%\n",
            "fasten:   \t0.06%\n",
            "euphrosyne:   \t0.06%\n",
            "inferno:   \t0.05%\n",
            "tatler:   \t0.05%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire straighten straighten speedily speedily \n",
            ":   \t99.97%\n",
            ".:   \t0.01%\n",
            ",:   \t0.01%\n",
            "and:   \t0.0%\n",
            "in:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire straighten straighten speedily speedily  \n",
            ":   \t99.97%\n",
            ".:   \t0.01%\n",
            ",:   \t0.01%\n",
            "and:   \t0.0%\n",
            "in:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"the meaning of life is\", max_tokens=30, temperature=0.9)\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prompt 2, Various Temperatures"
      ],
      "metadata": {
        "id": "EdGhxq9pUST6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LytOhBMZ32Mo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d0df63c-84a9-497b-acea-4d193738d0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 21539\n",
            "\n",
            "generated text:\n",
            "it was an awful cowardly cowardly privé” privé” voluminous voluminous ὲν ὲν\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly\n",
            "organ:   \t0.2%\n",
            "submerged:   \t0.15%\n",
            "suffrage:   \t0.13%\n",
            "opulent:   \t0.13%\n",
            "representative:   \t0.12%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly cowardly\n",
            "organ:   \t0.2%\n",
            "submerged:   \t0.15%\n",
            "suffrage:   \t0.13%\n",
            "opulent:   \t0.13%\n",
            "representative:   \t0.12%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly cowardly privé”\n",
            "morrow:   \t0.12%\n",
            "opulent:   \t0.07%\n",
            "allowances:   \t0.06%\n",
            "grossly:   \t0.06%\n",
            "much—everything—in:   \t0.06%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly cowardly privé” privé”\n",
            "morrow:   \t0.12%\n",
            "opulent:   \t0.07%\n",
            "allowances:   \t0.06%\n",
            "grossly:   \t0.06%\n",
            "much—everything—in:   \t0.06%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly cowardly privé” privé” voluminous\n",
            "footnote:   \t1.89%\n",
            "harry:   \t1.62%\n",
            "francis:   \t0.84%\n",
            "morrow:   \t0.58%\n",
            "enchanted:   \t0.51%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly cowardly privé” privé” voluminous voluminous\n",
            "footnote:   \t1.89%\n",
            "harry:   \t1.62%\n",
            "francis:   \t0.84%\n",
            "morrow:   \t0.58%\n",
            "enchanted:   \t0.51%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly cowardly privé” privé” voluminous voluminous ὲν\n",
            "morrow:   \t0.23%\n",
            "footnote:   \t0.14%\n",
            "euphrosyne:   \t0.1%\n",
            "fasten:   \t0.09%\n",
            "maternity:   \t0.09%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly cowardly privé” privé” voluminous voluminous ὲν ὲν\n",
            "morrow:   \t0.23%\n",
            "footnote:   \t0.14%\n",
            "euphrosyne:   \t0.1%\n",
            "fasten:   \t0.09%\n",
            "maternity:   \t0.09%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    \"it was an awful\", max_tokens=15, temperature=.6\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2Nhcu_8e34Hg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8e7249-6c09-49c4-a94c-c554fcde7e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\n",
            "generated text:\n",
            "it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _ . . . . . .\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged\n",
            "deal:   \t9.86%\n",
            "organ:   \t6.91%\n",
            "invalid:   \t2.17%\n",
            "adjustment:   \t1.72%\n",
            "commander:   \t1.71%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged\n",
            "deal:   \t9.86%\n",
            "organ:   \t6.91%\n",
            "invalid:   \t2.17%\n",
            "adjustment:   \t1.72%\n",
            "commander:   \t1.71%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45\n",
            "alluring:   \t0.97%\n",
            "impulsive:   \t0.56%\n",
            "marina:   \t0.52%\n",
            "austen’s:   \t0.5%\n",
            "pocked:   \t0.48%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45\n",
            "alluring:   \t0.97%\n",
            "impulsive:   \t0.56%\n",
            "marina:   \t0.52%\n",
            "austen’s:   \t0.5%\n",
            "pocked:   \t0.48%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his\n",
            "detect:   \t1.34%\n",
            "footnote:   \t1.23%\n",
            "ramble:   \t0.74%\n",
            "select:   \t0.59%\n",
            "edged:   \t0.55%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his\n",
            "detect:   \t1.34%\n",
            "footnote:   \t1.23%\n",
            "ramble:   \t0.74%\n",
            "select:   \t0.59%\n",
            "edged:   \t0.55%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer\n",
            "euphrosyne:   \t9.16%\n",
            "morrow:   \t4.35%\n",
            "religio:   \t3.43%\n",
            "tatler:   \t3.18%\n",
            "care—i:   \t3.18%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer\n",
            "euphrosyne:   \t9.16%\n",
            "morrow:   \t4.35%\n",
            "religio:   \t3.43%\n",
            "tatler:   \t3.18%\n",
            "care—i:   \t3.18%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted\n",
            "footnote:   \t10.53%\n",
            "dieu:   \t1.97%\n",
            "gather:   \t1.39%\n",
            "eyre:   \t1.08%\n",
            "flanks:   \t1.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted\n",
            "footnote:   \t10.53%\n",
            "dieu:   \t1.97%\n",
            "gather:   \t1.39%\n",
            "eyre:   \t1.08%\n",
            "flanks:   \t1.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged\n",
            "dine:   \t8.55%\n",
            "refuse:   \t6.26%\n",
            "detect:   \t3.93%\n",
            "ramble:   \t3.79%\n",
            "lap:   \t3.44%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged\n",
            "dine:   \t8.55%\n",
            "refuse:   \t6.26%\n",
            "detect:   \t3.93%\n",
            "ramble:   \t3.79%\n",
            "lap:   \t3.44%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch\n",
            "ramble:   \t2.17%\n",
            "euphrosyne:   \t1.96%\n",
            "edged:   \t1.91%\n",
            "fasten:   \t1.81%\n",
            "tatler:   \t1.48%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch\n",
            "ramble:   \t2.17%\n",
            "euphrosyne:   \t1.96%\n",
            "edged:   \t1.91%\n",
            "fasten:   \t1.81%\n",
            "tatler:   \t1.48%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after\n",
            "weepest:   \t4.72%\n",
            "footnote:   \t3.51%\n",
            "swee:   \t1.2%\n",
            "shaw:   \t1.03%\n",
            "marbot:   \t0.57%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after\n",
            "weepest:   \t4.72%\n",
            "footnote:   \t3.51%\n",
            "swee:   \t1.2%\n",
            "shaw:   \t1.03%\n",
            "marbot:   \t0.57%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained\n",
            "morrow:   \t8.13%\n",
            "minutes:   \t1.31%\n",
            "exist—“an:   \t0.97%\n",
            "allowances:   \t0.92%\n",
            "present”:   \t0.81%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained\n",
            "morrow:   \t8.13%\n",
            "minutes:   \t1.31%\n",
            "exist—“an:   \t0.97%\n",
            "allowances:   \t0.92%\n",
            "present”:   \t0.81%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield\n",
            "footnote:   \t16.63%\n",
            "detect:   \t1.74%\n",
            "criticise:   \t1.31%\n",
            "stoop:   \t1.2%\n",
            "apologise:   \t1.09%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield\n",
            "footnote:   \t16.63%\n",
            "detect:   \t1.74%\n",
            "criticise:   \t1.31%\n",
            "stoop:   \t1.2%\n",
            "apologise:   \t1.09%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative\n",
            "morrow:   \t8.54%\n",
            "obedience:   \t1.03%\n",
            "breeches:   \t0.88%\n",
            "stilt:   \t0.88%\n",
            "footnote:   \t0.56%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative\n",
            "morrow:   \t8.54%\n",
            "obedience:   \t1.03%\n",
            "breeches:   \t0.88%\n",
            "stilt:   \t0.88%\n",
            "footnote:   \t0.56%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook\n",
            "harry:   \t21.01%\n",
            "footnote:   \t17.86%\n",
            "allan:   \t3.97%\n",
            "francis:   \t2.8%\n",
            "morrow:   \t2.55%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook\n",
            "harry:   \t21.01%\n",
            "footnote:   \t17.86%\n",
            "allan:   \t3.97%\n",
            "francis:   \t2.8%\n",
            "morrow:   \t2.55%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow\n",
            "morrow:   \t39.39%\n",
            "footnote:   \t3.14%\n",
            "hustled:   \t2.53%\n",
            "supposed”:   \t1.19%\n",
            "austen’s:   \t0.74%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow\n",
            "morrow:   \t39.39%\n",
            "footnote:   \t3.14%\n",
            "hustled:   \t2.53%\n",
            "supposed”:   \t1.19%\n",
            "austen’s:   \t0.74%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate\n",
            "footnote:   \t17.55%\n",
            "conquest:   \t7.52%\n",
            "milan:   \t5.66%\n",
            "aspiration:   \t4.21%\n",
            "undergraduate:   \t3.5%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate\n",
            "footnote:   \t17.55%\n",
            "conquest:   \t7.52%\n",
            "milan:   \t5.66%\n",
            "aspiration:   \t4.21%\n",
            "undergraduate:   \t3.5%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate :\n",
            "::   \t99.99%\n",
            "rate:   \t0.01%\n",
            "elements:   \t0.0%\n",
            "extent:   \t0.0%\n",
            "importance:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : :\n",
            "::   \t99.99%\n",
            "rate:   \t0.01%\n",
            "elements:   \t0.0%\n",
            "extent:   \t0.0%\n",
            "importance:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _\n",
            ":   \t39.97%\n",
            "_:   \t13.3%\n",
            "she:   \t5.93%\n",
            "the:   \t5.06%\n",
            "was:   \t4.24%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _\n",
            ":   \t39.97%\n",
            "_:   \t13.3%\n",
            "she:   \t5.93%\n",
            "the:   \t5.06%\n",
            "was:   \t4.24%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a\n",
            "persuasion:   \t33.42%\n",
            "_:   \t19.86%\n",
            "times:   \t12.93%\n",
            "memoirs:   \t11.87%\n",
            "the:   \t6.22%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a\n",
            "persuasion:   \t33.42%\n",
            "_:   \t19.86%\n",
            "times:   \t12.93%\n",
            "memoirs:   \t11.87%\n",
            "the:   \t6.22%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china\n",
            "trestles:   \t2.76%\n",
            "voyage—china:   \t2.54%\n",
            "madrid:   \t1.04%\n",
            "charities:   \t1.02%\n",
            "rotherhithe:   \t0.9%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china\n",
            "trestles:   \t2.76%\n",
            "voyage—china:   \t2.54%\n",
            "madrid:   \t1.04%\n",
            "charities:   \t1.02%\n",
            "rotherhithe:   \t0.9%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici\n",
            "medici:   \t47.83%\n",
            "tales:   \t35.81%\n",
            "_:   \t12.18%\n",
            "review:   \t1.75%\n",
            "house:   \t1.23%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici\n",
            "medici:   \t47.83%\n",
            "tales:   \t35.81%\n",
            "_:   \t12.18%\n",
            "review:   \t1.75%\n",
            "house:   \t1.23%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _\n",
            "_:   \t85.85%\n",
            "the:   \t7.59%\n",
            ".:   \t5.86%\n",
            ",:   \t0.55%\n",
            "that:   \t0.07%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _\n",
            "_:   \t85.85%\n",
            "the:   \t7.59%\n",
            ".:   \t5.86%\n",
            ",:   \t0.55%\n",
            "that:   \t0.07%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _ .\n",
            ".:   \t86.54%\n",
            "?:   \t8.56%\n",
            ",:   \t4.88%\n",
            "_:   \t0.0%\n",
            ";:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _ . .\n",
            ".:   \t86.54%\n",
            "?:   \t8.56%\n",
            ",:   \t4.88%\n",
            "_:   \t0.0%\n",
            ";:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _ . . .\n",
            ".:   \t100.0%\n",
            ":   \t0.0%\n",
            "”:   \t0.0%\n",
            "i:   \t0.0%\n",
            "but:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _ . . . .\n",
            ".:   \t100.0%\n",
            ":   \t0.0%\n",
            "”:   \t0.0%\n",
            "i:   \t0.0%\n",
            "but:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _ . . . . .\n",
            ".:   \t98.69%\n",
            ":   \t1.3%\n",
            "”:   \t0.01%\n",
            "i:   \t0.0%\n",
            "he:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _ . . . . . .\n",
            ".:   \t98.69%\n",
            ":   \t1.3%\n",
            "”:   \t0.01%\n",
            "i:   \t0.0%\n",
            "he:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    \"it was an awful\", max_tokens=50, temperature=0.3\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"it was an awful\", max_tokens=15, temperature=0.1\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7MD1HVX6cPD",
        "outputId": "2cddb417-2cd4-4a7c-ba3f-cf83d582c320"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\n",
            "generated text:\n",
            "it was an awful opulent opulent tatler tatler footnote footnote register register austen’s austen’s footnote footnote\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent\n",
            "opulent:   \t99.17%\n",
            "elaborate:   \t0.2%\n",
            "iceblock:   \t0.11%\n",
            "submerged:   \t0.1%\n",
            "communs:   \t0.06%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent\n",
            "opulent:   \t99.17%\n",
            "elaborate:   \t0.2%\n",
            "iceblock:   \t0.11%\n",
            "submerged:   \t0.1%\n",
            "communs:   \t0.06%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler\n",
            "tatler:   \t63.69%\n",
            "euphrosyne:   \t14.78%\n",
            "odyssey:   \t6.82%\n",
            "religio:   \t6.82%\n",
            "inferno:   \t3.21%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler\n",
            "tatler:   \t63.69%\n",
            "euphrosyne:   \t14.78%\n",
            "odyssey:   \t6.82%\n",
            "religio:   \t6.82%\n",
            "inferno:   \t3.21%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote\n",
            "swee:   \t93.65%\n",
            "footnote:   \t6.12%\n",
            "weepest:   \t0.06%\n",
            "illustration:   \t0.04%\n",
            "hustled:   \t0.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote footnote\n",
            "swee:   \t93.65%\n",
            "footnote:   \t6.12%\n",
            "weepest:   \t0.06%\n",
            "illustration:   \t0.04%\n",
            "hustled:   \t0.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote footnote register\n",
            "trestles:   \t20.72%\n",
            "present”:   \t18.54%\n",
            "register:   \t11.93%\n",
            "had—it:   \t11.35%\n",
            "bees”:   \t1.82%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote footnote register register\n",
            "trestles:   \t20.72%\n",
            "present”:   \t18.54%\n",
            "register:   \t11.93%\n",
            "had—it:   \t11.35%\n",
            "bees”:   \t1.82%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote footnote register register austen’s\n",
            "austen’s:   \t76.87%\n",
            "eyre:   \t22.3%\n",
            "footnote:   \t0.42%\n",
            "colleagues:   \t0.32%\n",
            "flanks:   \t0.02%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote footnote register register austen’s austen’s\n",
            "austen’s:   \t76.87%\n",
            "eyre:   \t22.3%\n",
            "footnote:   \t0.42%\n",
            "colleagues:   \t0.32%\n",
            "flanks:   \t0.02%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote footnote register register austen’s austen’s footnote\n",
            "footnote:   \t55.89%\n",
            "morrow:   \t43.54%\n",
            "swee:   \t0.07%\n",
            "8:   \t0.06%\n",
            "7:   \t0.05%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote footnote register register austen’s austen’s footnote footnote\n",
            "footnote:   \t55.89%\n",
            "morrow:   \t43.54%\n",
            "swee:   \t0.07%\n",
            "8:   \t0.06%\n",
            "7:   \t0.05%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation of Text Generation with Single LSTM\n",
        "\n",
        "####The Single Layer LSTM performed poorly at generating coherent text resemblant of the style and tone of Virginia Woolf. It had a relatively short training time due to the simple nature of the architecture. However I was unable to make a single LSTM model capable of producing coherent text. I noticed that there are a lot of repeated words, likely due to overfitting. Even when I experimented with a dropout of various sizes, the issue was not resolved. I similarly did not see improvement when adjusting the temperature, batch size, layer size or experimenting with different prompts. The reason that some of the epochs say \"token out of range\" is because I implemented a vocabulay size of 25000 even though there were only about 18000 true terms in the dataset. Those cases are where the model sampled a token outside of the 18000 true dataset, even if the probability was very small."
      ],
      "metadata": {
        "id": "b7UnnP-efyQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi-Layer LSTM\n"
      ],
      "metadata": {
        "id": "ZaaRYzz3Thpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 20000\n",
        "MAX_LEN = 200\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 25"
      ],
      "metadata": {
        "id": "FtRLE_ctMfak"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x = layers.LSTM(256, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)(x)\n",
        "x = layers.LSTM(128, return_sequences=True, dropout=0.3, recurrent_dropout=0.2)(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "lstm_2 = models.Model(inputs, outputs)\n",
        "lstm_2.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 289
        },
        "id": "EmoSrWXjdOGP",
        "outputId": "9fe0b2bc-6126-4b92-fc83-0f24a4b3f8ef"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_3\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_3\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_3 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │       \u001b[38;5;34m2,000,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_6 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)           │         \u001b[38;5;34m365,568\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_7 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m197,120\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)         │       \u001b[38;5;34m2,580,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_6 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">365,568</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_7 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">197,120</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,580,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,142,688\u001b[0m (19.62 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,142,688</span> (19.62 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,142,688\u001b[0m (19.62 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,142,688</span> (19.62 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Multi-Layer LSTM"
      ],
      "metadata": {
        "id": "iNCvafOAUZh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = losses.SparseCategoricalCrossentropy()\n",
        "lstm_2.compile(\"adam\", loss_fn)"
      ],
      "metadata": {
        "id": "S_7oXItCTlCl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TextGenerator checkpoint\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index\n",
        "            for index, word in enumerate(index_to_word)\n",
        "        }\n",
        "\n",
        "    def sample_from(self, probs, temperature):\n",
        "        if isinstance(probs, (float, np.float64)):  # Check if probs is a single value\n",
        "            probs = np.array([probs, 1 - probs])  # Create a 2-element distribution\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
        "            y = self.model.predict(np.array([start_tokens]))\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
        "            if 0 <= sample_token < len(self.index_to_word):  # Check if sample_token is within range\n",
        "              start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "              info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "              start_tokens.append(sample_token)\n",
        "            else:\n",
        "              # Handle case where sample_token is out of range\n",
        "              print(f\"Warning: sample_token out of range: {sample_token}\")\n",
        "              break\n",
        "\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      try:\n",
        "        prompts = ('the meaning of life is', 'it is an awful')\n",
        "        prompt = np.random.choice(prompts)\n",
        "        self.generate(prompt, max_tokens=100, temperature=1.0)\n",
        "      except Exception as e:\n",
        "        print(f\"Error during text generation: {e}\")"
      ],
      "metadata": {
        "id": "hiIeZ8nDTlGH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize starting prompt\n",
        "text_generator = TextGenerator(vocab)"
      ],
      "metadata": {
        "id": "ejwg7705Uhvb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_2.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[text_generator],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "88Fqz8kSdcs6",
        "outputId": "0d7b1166-96eb-46e8-a898-185832d1a1f2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m1s\u001b[0m 597ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 318ms/step\n",
            "\n",
            "generated text:\n",
            "it is an awful months \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m223s\u001b[0m 558ms/step - loss: 2.6015\n",
            "Epoch 2/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 45ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "Warning: sample_token out of range: 19655\n",
            "\n",
            "generated text:\n",
            "it is an awful envisaging works” frankly repels bores anywhere ful moreover—” περιβρυχίοισι simplify tapped nothing—no withdrew extend worth monotonous enchanted lids the\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 553ms/step - loss: 0.6040\n",
            "Epoch 3/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "Warning: sample_token out of range: 18871\n",
            "\n",
            "generated text:\n",
            "the meaning of life is jingle range worryin’ souls rude compensations courts” meat” venning\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 548ms/step - loss: 0.6015\n",
            "Epoch 4/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Warning: sample_token out of range: 19707\n",
            "\n",
            "generated text:\n",
            "the meaning of life is omission main neutral satisfaction appearing asperity\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 547ms/step - loss: 0.5976\n",
            "Epoch 5/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Warning: sample_token out of range: 19799\n",
            "\n",
            "generated text:\n",
            "it is an awful preserved within unrest comforts move swearing αἰ\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m210s\u001b[0m 543ms/step - loss: 0.5837\n",
            "Epoch 6/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 59ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 60ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "Warning: sample_token out of range: 19170\n",
            "\n",
            "generated text:\n",
            "it is an awful garden disputing pleaded life” opinion amiable rhubarbarum stretches morleys “matthew ancients wit fame” burglar alleys knit conveyed chronicle send fiddlers neither imitating elusive colonus mountaineers\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 555ms/step - loss: 0.5644\n",
            "Epoch 7/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Warning: sample_token out of range: 19191\n",
            "\n",
            "generated text:\n",
            "it is an awful keys comparative querulous architect’s\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m229s\u001b[0m 591ms/step - loss: 0.5510\n",
            "Epoch 8/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "Warning: sample_token out of range: 19511\n",
            "\n",
            "generated text:\n",
            "it is an awful furs liked other’s walks nursery forecast prize\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m258s\u001b[0m 583ms/step - loss: 0.5388\n",
            "Epoch 9/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life is later \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 551ms/step - loss: 0.5309\n",
            "Epoch 10/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "Warning: sample_token out of range: 18977\n",
            "\n",
            "generated text:\n",
            "it is an awful hyperbole\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 546ms/step - loss: 0.5148\n",
            "Epoch 11/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 18968\n",
            "\n",
            "generated text:\n",
            "it is an awful\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m260s\u001b[0m 542ms/step - loss: 0.5075\n",
            "Epoch 12/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Warning: sample_token out of range: 19475\n",
            "\n",
            "generated text:\n",
            "the meaning of life is landlords reins edifice\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 546ms/step - loss: 0.4992\n",
            "Epoch 13/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life is stapletons jealously \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 547ms/step - loss: 0.4934\n",
            "Epoch 14/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Warning: sample_token out of range: 19027\n",
            "\n",
            "generated text:\n",
            "it is an awful ruffles xx paws “in lawrence indefatigable says—“there wheels raked blurring disappears evident tippets diffident gnawing childish powers temperature tyres limp disappearing walken temptation\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 547ms/step - loss: 0.4832\n",
            "Epoch 15/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 62ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 65ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 54ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 61ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 94ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 67ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 68ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 76ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 79ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 77ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 87ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 101ms/step\n",
            "Warning: sample_token out of range: 19389\n",
            "\n",
            "generated text:\n",
            "the meaning of life is spotted union caressing daily felled addition inspirer overrun chaucer’s terriers creepers perambulating l’ont doris lanthorn nature’s plucking selfish ascended strife starlit stands trivial unbordered suffocation roar fluency indefatigable cynicism unredeemed dusk laughed tomlinsons’ anticipating answer stitching “writing\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m221s\u001b[0m 570ms/step - loss: 0.4776\n",
            "Epoch 16/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 19274\n",
            "\n",
            "generated text:\n",
            "it is an awful\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m252s\u001b[0m 544ms/step - loss: 0.4725\n",
            "Epoch 17/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "Warning: sample_token out of range: 19973\n",
            "\n",
            "generated text:\n",
            "it is an awful moly collect ceremony term’s locking solemnly rejoined tie officer woman—all\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m211s\u001b[0m 545ms/step - loss: 0.4667\n",
            "Epoch 18/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "Warning: sample_token out of range: 18759\n",
            "\n",
            "generated text:\n",
            "it is an awful casterbridge notable verandah complacently chronicle listening dulness novel—a darted theatre\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m216s\u001b[0m 558ms/step - loss: 0.4628\n",
            "Epoch 19/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life is \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 548ms/step - loss: 0.4593\n",
            "Epoch 20/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 19683\n",
            "\n",
            "generated text:\n",
            "it is an awful ants\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 552ms/step - loss: 0.4488\n",
            "Epoch 21/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Warning: sample_token out of range: 19575\n",
            "\n",
            "generated text:\n",
            "the meaning of life is\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 548ms/step - loss: 0.4478\n",
            "Epoch 22/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "Warning: sample_token out of range: 19964\n",
            "\n",
            "generated text:\n",
            "it is an awful on—going goad\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m213s\u001b[0m 550ms/step - loss: 0.4441\n",
            "Epoch 23/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 48ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 66ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 64ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "Warning: sample_token out of range: 18956\n",
            "\n",
            "generated text:\n",
            "the meaning of life is refine gigantic labours nethermost brighter lemon etc perplexed civil survival readiness fable respectability area clasped schipperke chocolates shortest dean hops diffuse 1919\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m214s\u001b[0m 553ms/step - loss: 0.4421\n",
            "Epoch 24/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "Warning: sample_token out of range: 19681\n",
            "\n",
            "generated text:\n",
            "the meaning of life is contentedly obscurities humanity betty embroidered example—are lord’s clamouring beaver\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m212s\u001b[0m 547ms/step - loss: 0.4340\n",
            "Epoch 25/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life is \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m215s\u001b[0m 555ms/step - loss: 0.4323\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7ae9260fc490>"
            ]
          },
          "metadata": {},
          "execution_count": 47
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Generation\n",
        "##with Multi-Layer LSTM"
      ],
      "metadata": {
        "id": "m6fUxoT3UqRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            if 0 <= i < len(vocab):\n",
        "                print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "            else:\n",
        "                print(f\"Index {i} out of range for vocabulary (size: {len(vocab)})\") # Print error message\n",
        "        print(\"--------\\n\")"
      ],
      "metadata": {
        "id": "hxdF360LUpjO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prompt 1 with Various Temperatures"
      ],
      "metadata": {
        "id": "NtsUu6ZqVDIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"it was a dark and stormy night when\", max_tokens=20, temperature=1.0\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "flkRxvfTVF3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7d56bd8c-0922-48dc-8a69-b24a28deb3a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 36ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 57ms/step\n",
            "Warning: sample_token out of range: 18708\n",
            "\n",
            "generated text:\n",
            "it was a dark and stormy night when cambridge loses cried truthful masters confounds passage browne author\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when cambridge\n",
            "censure:   \t0.01%\n",
            "girls—nothing:   \t0.01%\n",
            "morrow:   \t0.01%\n",
            "legions:   \t0.01%\n",
            "despite:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when cambridge loses\n",
            "distressed:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "provisional:   \t0.01%\n",
            "duration:   \t0.01%\n",
            "thoroughgoing:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when cambridge loses cried\n",
            "morrow:   \t0.01%\n",
            "smith’s:   \t0.01%\n",
            "me—all:   \t0.01%\n",
            "wales:   \t0.01%\n",
            "swine:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when cambridge loses cried truthful\n",
            "distressed:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "mighty:   \t0.01%\n",
            "morrow:   \t0.01%\n",
            "“papa:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when cambridge loses cried truthful masters\n",
            "“papa:   \t0.01%\n",
            "“too:   \t0.01%\n",
            "medieval:   \t0.01%\n",
            "tutors:   \t0.01%\n",
            "authoritatively:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when cambridge loses cried truthful masters confounds\n",
            "“papa:   \t0.01%\n",
            "“too:   \t0.01%\n",
            "medieval:   \t0.01%\n",
            "tutors:   \t0.01%\n",
            "“richard:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when cambridge loses cried truthful masters confounds passage\n",
            "jove:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "monmouth:   \t0.01%\n",
            "vols:   \t0.01%\n",
            "hastening:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when cambridge loses cried truthful masters confounds passage browne\n",
            "“papa:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "distressed:   \t0.01%\n",
            "“too:   \t0.01%\n",
            "tutors:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when cambridge loses cried truthful masters confounds passage browne author\n",
            "“papa:   \t0.01%\n",
            "“too:   \t0.01%\n",
            "authoritatively:   \t0.01%\n",
            "“about:   \t0.01%\n",
            "tutors:   \t0.01%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"it was a dark and stormy night when\", max_tokens=20, temperature=0.8\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "9HcfXkLvVGvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cf04f050-175d-4781-e8df-fe8c7ee888a2"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Warning: sample_token out of range: 19775\n",
            "\n",
            "generated text:\n",
            "it was a dark and stormy night when hid partner’s child’s traveller soul—calls robin\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when hid\n",
            "censure:   \t0.01%\n",
            "girls—nothing:   \t0.01%\n",
            "morrow:   \t0.01%\n",
            "legions:   \t0.01%\n",
            "despite:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when hid partner’s\n",
            "morrow:   \t0.01%\n",
            "gissing:   \t0.01%\n",
            "girls—nothing:   \t0.01%\n",
            "censure:   \t0.01%\n",
            "pine:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when hid partner’s child’s\n",
            "swine:   \t0.01%\n",
            "aspire:   \t0.01%\n",
            "me—all:   \t0.01%\n",
            "smith’s:   \t0.01%\n",
            "quip:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when hid partner’s child’s traveller\n",
            "jove:   \t0.01%\n",
            "detract:   \t0.01%\n",
            "embitter:   \t0.01%\n",
            "mayors—what:   \t0.01%\n",
            "enviable:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when hid partner’s child’s traveller soul—calls\n",
            "“papa:   \t0.01%\n",
            "“too:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "tutors:   \t0.01%\n",
            "authoritatively:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when hid partner’s child’s traveller soul—calls robin\n",
            "jove:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "monmouth:   \t0.01%\n",
            "“papa:   \t0.01%\n",
            "mighty:   \t0.01%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"it was a dark and stormy night when\", max_tokens=20, temperature=.5\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "0OkCuC4JVG0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6753a6e9-f914-4f04-aa9b-2811f6a6cd41"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Warning: sample_token out of range: 18946\n",
            "\n",
            "generated text:\n",
            "it was a dark and stormy night when affection—all disapproval stucco literalness queerness looking herb nasal guns ridicule\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when affection—all\n",
            "censure:   \t0.01%\n",
            "girls—nothing:   \t0.01%\n",
            "morrow:   \t0.01%\n",
            "legions:   \t0.01%\n",
            "despite:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when affection—all disapproval\n",
            "jove:   \t0.01%\n",
            "hastening:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "heaven’s:   \t0.01%\n",
            "monmouth:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when affection—all disapproval stucco\n",
            "distressed:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "jove:   \t0.01%\n",
            "accurate:   \t0.01%\n",
            "hemmed:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when affection—all disapproval stucco literalness\n",
            "jove:   \t0.01%\n",
            "hemmed:   \t0.01%\n",
            "vols:   \t0.01%\n",
            "hastening:   \t0.01%\n",
            "heaven’s:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when affection—all disapproval stucco literalness queerness\n",
            "barber’s:   \t0.01%\n",
            "distressed:   \t0.01%\n",
            "unvexed:   \t0.01%\n",
            "jove:   \t0.01%\n",
            "cinder:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when affection—all disapproval stucco literalness queerness looking\n",
            "distressed:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "thoroughgoing:   \t0.01%\n",
            "provisional:   \t0.01%\n",
            "“papa:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when affection—all disapproval stucco literalness queerness looking herb\n",
            "jove:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "vols:   \t0.01%\n",
            "hemmed:   \t0.01%\n",
            "distressed:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when affection—all disapproval stucco literalness queerness looking herb nasal\n",
            "barber’s:   \t0.01%\n",
            "morrow:   \t0.01%\n",
            "“papa:   \t0.01%\n",
            "distressed:   \t0.01%\n",
            "mighty:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when affection—all disapproval stucco literalness queerness looking herb nasal guns\n",
            "quip:   \t0.01%\n",
            "aspire:   \t0.01%\n",
            "morrow:   \t0.01%\n",
            "endow:   \t0.01%\n",
            "masquerade:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when affection—all disapproval stucco literalness queerness looking herb nasal guns ridicule\n",
            "barber’s:   \t0.01%\n",
            "morrow:   \t0.01%\n",
            "distressed:   \t0.01%\n",
            "“papa:   \t0.01%\n",
            "locust:   \t0.01%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"it was a dark and stormy night when\", max_tokens=20, temperature=.2\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WASehKy-zykx",
        "outputId": "5dc39043-a4f1-4dd3-dc7d-116f2e4f3648"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 40ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 63ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 50ms/step\n",
            "Warning: sample_token out of range: 19482\n",
            "\n",
            "generated text:\n",
            "it was a dark and stormy night when noises qualifications pay gibbering resemblances unenviable\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when noises\n",
            "censure:   \t0.03%\n",
            "girls—nothing:   \t0.03%\n",
            "morrow:   \t0.03%\n",
            "legions:   \t0.03%\n",
            "despite:   \t0.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when noises qualifications\n",
            "distressed:   \t0.02%\n",
            "“papa:   \t0.02%\n",
            "duration:   \t0.02%\n",
            "barber’s:   \t0.02%\n",
            "microscopic:   \t0.02%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when noises qualifications pay\n",
            "distressed:   \t0.03%\n",
            "barber’s:   \t0.03%\n",
            "jove:   \t0.02%\n",
            "hemmed:   \t0.02%\n",
            "accurate:   \t0.02%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when noises qualifications pay gibbering\n",
            "morrow:   \t0.03%\n",
            "barber’s:   \t0.03%\n",
            "“papa:   \t0.02%\n",
            "mighty:   \t0.02%\n",
            "“too:   \t0.02%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when noises qualifications pay gibbering resemblances\n",
            "distressed:   \t0.03%\n",
            "barber’s:   \t0.02%\n",
            "“papa:   \t0.02%\n",
            "mighty:   \t0.02%\n",
            "“too:   \t0.02%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dark and stormy night when noises qualifications pay gibbering resemblances unenviable\n",
            "distressed:   \t0.03%\n",
            "barber’s:   \t0.03%\n",
            "provisional:   \t0.02%\n",
            "thoroughgoing:   \t0.02%\n",
            "accurate:   \t0.02%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prompt 2 with Various Temperatures"
      ],
      "metadata": {
        "id": "YydmisQzVILV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"men must not\", max_tokens=10, temperature=1.0\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "jYM44EVrVWaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f617c093-1665-439c-93f8-23799b6e4f46"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\n",
            "generated text:\n",
            "men must not titles treacle again—the inquest bird’s ode arrange\n",
            "\n",
            "\n",
            "PROMPT: men must not titles\n",
            "charmin’:   \t0.11%\n",
            "drenching:   \t0.11%\n",
            "assistant:   \t0.09%\n",
            "epitome:   \t0.09%\n",
            "boétie:   \t0.09%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not titles treacle\n",
            "charmin’:   \t0.1%\n",
            "assistant:   \t0.1%\n",
            "boétie:   \t0.09%\n",
            "epitome:   \t0.09%\n",
            "drenching:   \t0.09%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not titles treacle again—the\n",
            "hood:   \t0.03%\n",
            "uneven:   \t0.03%\n",
            "recent:   \t0.02%\n",
            "playthings:   \t0.02%\n",
            "formlessness:   \t0.02%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not titles treacle again—the inquest\n",
            "ostler:   \t0.01%\n",
            "playthings:   \t0.01%\n",
            "minuteness:   \t0.01%\n",
            "footnote:   \t0.01%\n",
            "uneven:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not titles treacle again—the inquest bird’s\n",
            "evaporation:   \t0.01%\n",
            "jove:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "asleep”:   \t0.01%\n",
            "awarded:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not titles treacle again—the inquest bird’s ode\n",
            "distressed:   \t0.01%\n",
            "evaporation:   \t0.01%\n",
            "conceitedness:   \t0.01%\n",
            "mighty:   \t0.01%\n",
            "hawklike:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not titles treacle again—the inquest bird’s ode arrange\n",
            "“papa:   \t0.01%\n",
            "evaporation:   \t0.01%\n",
            "mighty:   \t0.01%\n",
            "distressed:   \t0.01%\n",
            "“too:   \t0.01%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"men must not\", max_tokens=10, temperature=.75\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "tVduzLgXVOZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "13a8b1d0-b02b-4898-d627-f6bc48a16bb3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\n",
            "generated text:\n",
            "men must not back part traces reformer’s something’s bats—“odious waft\n",
            "\n",
            "\n",
            "PROMPT: men must not back\n",
            "charmin’:   \t0.27%\n",
            "drenching:   \t0.26%\n",
            "assistant:   \t0.2%\n",
            "epitome:   \t0.19%\n",
            "boétie:   \t0.18%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not back part\n",
            "drenching:   \t0.11%\n",
            "formlessness:   \t0.11%\n",
            "charmin’:   \t0.11%\n",
            "epitome:   \t0.1%\n",
            "grossest:   \t0.09%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not back part traces\n",
            "footnote:   \t0.02%\n",
            "minuteness:   \t0.02%\n",
            "formlessness:   \t0.01%\n",
            "“general:   \t0.01%\n",
            "recent:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not back part traces reformer’s\n",
            "evaporation:   \t0.01%\n",
            "conceitedness:   \t0.01%\n",
            "opulent:   \t0.01%\n",
            "distressed:   \t0.01%\n",
            "footnote:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not back part traces reformer’s something’s\n",
            "encountered:   \t0.01%\n",
            "footnote:   \t0.01%\n",
            "xxiii:   \t0.01%\n",
            "ascertained:   \t0.01%\n",
            "enviable:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not back part traces reformer’s something’s bats—“odious\n",
            "masquerade:   \t0.01%\n",
            "aspire:   \t0.01%\n",
            "sharpen:   \t0.01%\n",
            "curtsey:   \t0.01%\n",
            "stifle:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not back part traces reformer’s something’s bats—“odious waft\n",
            "jove:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "possessing:   \t0.01%\n",
            "stikkes:   \t0.01%\n",
            "copulation:   \t0.01%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"men must not\", max_tokens=15, temperature=.5\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fzgpzWIt1z6n",
        "outputId": "1450322e-b7ac-409a-f021-7831985820f8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\n",
            "generated text:\n",
            "men must not great peeling better—this superstitious “burke crinkling rome—himself bundling filmy stifle proceeding bats—“odious\n",
            "\n",
            "\n",
            "PROMPT: men must not great\n",
            "charmin’:   \t1.07%\n",
            "drenching:   \t1.0%\n",
            "assistant:   \t0.67%\n",
            "epitome:   \t0.63%\n",
            "boétie:   \t0.61%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not great peeling\n",
            "drenching:   \t0.58%\n",
            "charmin’:   \t0.47%\n",
            "assistant:   \t0.43%\n",
            "epitome:   \t0.41%\n",
            "hood:   \t0.39%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not great peeling better—this\n",
            "formlessness:   \t0.06%\n",
            "foggy:   \t0.05%\n",
            "grossest:   \t0.05%\n",
            "thing—isn’t:   \t0.05%\n",
            "minuteness:   \t0.05%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not great peeling better—this superstitious\n",
            "evaporation:   \t0.02%\n",
            "“general:   \t0.02%\n",
            "minuteness:   \t0.02%\n",
            "adept:   \t0.02%\n",
            "alluring:   \t0.02%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not great peeling better—this superstitious “burke\n",
            "footnote:   \t0.01%\n",
            "asleep”:   \t0.01%\n",
            "gratuitously:   \t0.01%\n",
            "advisable:   \t0.01%\n",
            "unheard:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not great peeling better—this superstitious “burke crinkling\n",
            "distressed:   \t0.01%\n",
            "“papa:   \t0.01%\n",
            "evaporation:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "footnote:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not great peeling better—this superstitious “burke crinkling rome—himself\n",
            "distressed:   \t0.01%\n",
            "“papa:   \t0.01%\n",
            "mighty:   \t0.01%\n",
            "depressing:   \t0.01%\n",
            "“too:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not great peeling better—this superstitious “burke crinkling rome—himself bundling\n",
            "morrow:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "me—all:   \t0.01%\n",
            "copulation:   \t0.01%\n",
            "europeans:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not great peeling better—this superstitious “burke crinkling rome—himself bundling filmy\n",
            "“papa:   \t0.01%\n",
            "“too:   \t0.01%\n",
            "tutors:   \t0.01%\n",
            "authoritatively:   \t0.01%\n",
            "“about:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not great peeling better—this superstitious “burke crinkling rome—himself bundling filmy stifle\n",
            "footnote:   \t0.01%\n",
            "“papa:   \t0.01%\n",
            "quip:   \t0.01%\n",
            "morrow:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not great peeling better—this superstitious “burke crinkling rome—himself bundling filmy stifle proceeding\n",
            "morrow:   \t0.02%\n",
            "harassing:   \t0.01%\n",
            "me—all:   \t0.01%\n",
            "perplex:   \t0.01%\n",
            "pine:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not great peeling better—this superstitious “burke crinkling rome—himself bundling filmy stifle proceeding bats—“odious\n",
            "jove:   \t0.01%\n",
            "possessing:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "precedent:   \t0.01%\n",
            "unheard:   \t0.01%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"men must not\", max_tokens=10, temperature=.15\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ehO80SRR0mxZ",
        "outputId": "023bc70f-832a-4101-b723-301fb02ac638"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\n",
            "generated text:\n",
            "men must not charm recent massacres friends—though footnote footnote entrails\n",
            "\n",
            "\n",
            "PROMPT: men must not charm\n",
            "charmin’:   \t30.56%\n",
            "drenching:   \t24.35%\n",
            "assistant:   \t6.41%\n",
            "epitome:   \t5.19%\n",
            "boétie:   \t4.74%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not charm recent\n",
            "formlessness:   \t10.65%\n",
            "charmin’:   \t10.21%\n",
            "drenching:   \t9.71%\n",
            "epitome:   \t7.49%\n",
            "grossest:   \t4.81%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not charm recent massacres\n",
            "footnote:   \t13.61%\n",
            "8:   \t3.92%\n",
            "13:   \t1.76%\n",
            "insatiable:   \t1.22%\n",
            "ally:   \t1.07%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not charm recent massacres friends—though\n",
            "footnote:   \t86.47%\n",
            "illustration:   \t0.45%\n",
            "superseded:   \t0.44%\n",
            "horrified:   \t0.39%\n",
            "8:   \t0.21%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not charm recent massacres friends—though footnote\n",
            "footnote:   \t91.48%\n",
            "illustration:   \t0.34%\n",
            "superseded:   \t0.16%\n",
            "horrified:   \t0.14%\n",
            "secular:   \t0.1%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not charm recent massacres friends—though footnote footnote\n",
            "footnote:   \t77.36%\n",
            "illustration:   \t2.23%\n",
            "divinity:   \t0.45%\n",
            "pilgrims:   \t0.24%\n",
            "impossibility:   \t0.22%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: men must not charm recent massacres friends—though footnote footnote entrails\n",
            "footnote:   \t13.99%\n",
            "management:   \t5.74%\n",
            "innkeeper:   \t5.36%\n",
            "impossibility:   \t3.09%\n",
            "organism:   \t2.63%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "###Prompt 3, Various Temperatures"
      ],
      "metadata": {
        "id": "ZDIoct051Mq7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"it was an awful\", max_tokens=10, temperature=.8\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "3-ONTd2dVNsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bc77b350-19c7-4773-811b-671edf619dd8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "Warning: sample_token out of range: 18683\n",
            "\n",
            "generated text:\n",
            "it was an awful tulliver barbarian\n",
            "\n",
            "\n",
            "PROMPT: it was an awful tulliver\n",
            "walsh’s:   \t0.02%\n",
            "“cabbage:   \t0.01%\n",
            "coagulate:   \t0.01%\n",
            "mayors—what:   \t0.01%\n",
            "speedily:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful tulliver barbarian\n",
            "encountered:   \t0.01%\n",
            "“cabbage:   \t0.01%\n",
            "peeled:   \t0.01%\n",
            "stooped:   \t0.01%\n",
            "footnote:   \t0.01%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"it was an awful\", max_tokens=15, temperature=.4\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b7snMZCB1Pfv",
        "outputId": "dc5017b8-0958-47fd-d62a-a9199113aefe"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\n",
            "generated text:\n",
            "it was an awful satisfactorily damaged men—but carters flow tough lust sadly kitten doted crossways\n",
            "\n",
            "\n",
            "PROMPT: it was an awful satisfactorily\n",
            "walsh’s:   \t0.04%\n",
            "“cabbage:   \t0.04%\n",
            "coagulate:   \t0.04%\n",
            "mayors—what:   \t0.03%\n",
            "speedily:   \t0.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful satisfactorily damaged\n",
            "vols:   \t0.03%\n",
            "him—a:   \t0.03%\n",
            "stationed:   \t0.03%\n",
            "unheard:   \t0.03%\n",
            "sylvia:   \t0.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful satisfactorily damaged men—but\n",
            "jove:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "evaporation:   \t0.01%\n",
            "him—a:   \t0.01%\n",
            "awarded:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful satisfactorily damaged men—but carters\n",
            "barber’s:   \t0.01%\n",
            "jove:   \t0.01%\n",
            "stationed:   \t0.01%\n",
            "“cabbage:   \t0.01%\n",
            "awarded:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful satisfactorily damaged men—but carters flow\n",
            "distressed:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "mighty:   \t0.01%\n",
            "evaporation:   \t0.01%\n",
            "morrow:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful satisfactorily damaged men—but carters flow tough\n",
            "distressed:   \t0.02%\n",
            "barber’s:   \t0.01%\n",
            "thoroughgoing:   \t0.01%\n",
            "comely:   \t0.01%\n",
            "“papa:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful satisfactorily damaged men—but carters flow tough lust\n",
            "morrow:   \t0.02%\n",
            "barber’s:   \t0.01%\n",
            "pine:   \t0.01%\n",
            "me—all:   \t0.01%\n",
            "censure:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful satisfactorily damaged men—but carters flow tough lust sadly\n",
            "distressed:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "“papa:   \t0.01%\n",
            "“too:   \t0.01%\n",
            "evaporation:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful satisfactorily damaged men—but carters flow tough lust sadly kitten\n",
            "morrow:   \t0.02%\n",
            "barber’s:   \t0.02%\n",
            "censure:   \t0.01%\n",
            "me—all:   \t0.01%\n",
            "quip:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful satisfactorily damaged men—but carters flow tough lust sadly kitten doted\n",
            "distressed:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "censure:   \t0.01%\n",
            "destroying:   \t0.01%\n",
            "xxiii:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful satisfactorily damaged men—but carters flow tough lust sadly kitten doted crossways\n",
            "distressed:   \t0.01%\n",
            "barber’s:   \t0.01%\n",
            "“papa:   \t0.01%\n",
            "cadence:   \t0.01%\n",
            "comely:   \t0.01%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation of Text Generation with Multi-Layer LSTM\n",
        "\n",
        "The Multi-Layer LSTM is more sophisticated than the Single-Layer LSTM, though there are still many flaws in syntax and semantics.\n",
        "\n",
        "I attempted many variations of the multi-layer LSTM. The biggest obstacle I faced was word repetition of generated text. When training, I had sentences in which every word was repeated once, for example: \"the meaning of life is prove prove very very , , footnote footnote\". I tried increasing the dropout rate of the LSTM layers and adding recurrent drop out layers to prevent overfitting of any repetition that may have been in the books. I experimented with layers of varying sizes and different types of prompts. I also increased the temperature for more varied text. The only thing that changed the repetitive nature was when increasing the temperature all the way to 1.0 in the training process.\n",
        "\n",
        "In this model, the generated text uses punctuation far more accurately, albeit mistakes, than the single-layer LSTM. It did not have excessive repetition. The model seemed to have picked up some associations between words. For example, for the \"dark and stormy night\" prompt, it generally produced words that were more negative, scary, gloomy, or gory. It still lacked in generating sentences with a coherent meaning. I found that portions of the generated sentences were semantically consistent, but as a whole the lines generally were not comprehendable.\n",
        "\n",
        "In further experiments, I would like to train the LSTM with more layers and adding another book in the dataset. This may require a lot more training time than I can access before being booted."
      ],
      "metadata": {
        "id": "avttg4agf2Xd"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "mJ9DJyoq1wUJ",
        "TZsUQObW2pgu",
        "61bA3bh6216j",
        "go6WtJ5I3mzu"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}