{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "collapsed_sections": [
        "mJ9DJyoq1wUJ",
        "TZsUQObW2pgu",
        "go6WtJ5I3mzu"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KelseyNager/GenAI/blob/main/Assignment_5b.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#LSTM\n",
        "##Kelsey Nager\n",
        "##CSC 330"
      ],
      "metadata": {
        "id": "FSrjyb8l1uzT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, losses"
      ],
      "metadata": {
        "id": "P2rzFk5-rblV"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "VOCAB_SIZE = 10000\n",
        "MAX_LEN = 200\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 25"
      ],
      "metadata": {
        "id": "U3egQ336rwa_"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#1 Data Collection and Preparation"
      ],
      "metadata": {
        "id": "mJ9DJyoq1wUJ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "CfqXZdFdp4C7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a914d118-4b47-48d9-9189-098b4c690259"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start match found: True\n",
            "End match found: True\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import re\n",
        "\n",
        "\n",
        "def trim_book_content(book_content, start, end):\n",
        "    \"\"\"Trims the beginning and end of book content using markers.\"\"\"\n",
        "    start_match = re.search(re.escape(start), book_content)\n",
        "    end_match = re.search(re.escape(end), book_content)\n",
        "\n",
        "    print(f\"Start match found: {start_match is not None}\")  # Check if start marker is found\n",
        "    print(f\"End match found: {end_match is not None}\")    # Check if end marker is found\n",
        "\n",
        "    if start_match and end_match:\n",
        "        start_index = start_match.end()\n",
        "        end_index = end_match.start()\n",
        "        trimmed_content = book_content[start_index:end_index]\n",
        "        return trimmed_content\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "# Download each text file and append to all_books\n",
        "urls = [\n",
        "    \"https://www.gutenberg.org/cache/epub/84/pg84.txt\" #Frankenstein, Mary Shelley\n",
        "  #\"https://www.gutenberg.org/cache/epub/71865/pg71865.txt\"  # Mrs Dalloway, Virginia Woolf\n",
        "#  \"https://www.gutenberg.org/cache/epub/29220/pg29220.txt\",   # Monday or Tuesday, Virginia Woolf\n",
        " # \"https://www.gutenberg.org/cache/epub/64457/pg64457.txt\"   # The Common Reader,\n",
        "      ]\n",
        "\n",
        "start = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "end = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "all_books = \"\"\n",
        "\n",
        "# Save combined text to a single file\n",
        "for url in urls:\n",
        "  response = requests.get(url)\n",
        "  book_content = response.text\n",
        "  trimmed_text = trim_book_content(book_content, start, end)\n",
        "  all_books += trimmed_text + \"\\n\\n\"\n",
        "\n",
        "with open('all_books_trimmed.txt', 'w', encoding='utf-8') as outfile:\n",
        "    outfile.write(all_books)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import string\n",
        "\n",
        "def pad_punctuation(s):\n",
        "    s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s)  # Pad punctuation\n",
        "    s = re.sub(\" +\", \" \", s)\n",
        "    s = s.lower()  # Convert to lowercase for consistency\n",
        "    return s.split()  # Split into words\n",
        "\n",
        "text_data = [pad_punctuation(s) for s in all_books.split()]"
      ],
      "metadata": {
        "id": "70pfkORxsJYh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display an example word\n",
        "example_data = text_data[10]\n",
        "example_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "KAvTF1SsK4eY",
        "outputId": "3033dcf7-d34c-4292-848d-53c5da754d0a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['prometheus']"
            ]
          },
          "metadata": {},
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(f\"Length of filtered_data: {len(text_data)}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-UOtajoL6oi",
        "outputId": "d4e67706-67b2-4241-b13f-745329dae961"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Length of filtered_data: 75048\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Convert to a Tensorflow Dataset\n",
        "flattened_text_data = [word for sublist in text_data for word in sublist]\n",
        "\n",
        "text_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(flattened_text_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(1000)\n",
        ")"
      ],
      "metadata": {
        "id": "mr9FuN79sKT6"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a vectorization layer\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower\",\n",
        "    max_tokens=VOCAB_SIZE,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LEN + 1,\n",
        ")"
      ],
      "metadata": {
        "id": "ZwRkI5HbsKXb"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Adapt the layer to the training set\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()"
      ],
      "metadata": {
        "id": "7jgxCJ6XsR30"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Display some token:word mappings\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "    print(f\"{i}: {word}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1uQdQAa65ga",
        "outputId": "bccb71e8-3218-468e-b512-88c0b0cfe6bf"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \n",
            "1: [UNK]\n",
            "2: ,\n",
            "3: the\n",
            "4: and\n",
            "5: .\n",
            "6: i\n",
            "7: of\n",
            "8: to\n",
            "9: my\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Create the training set of book content and the same text shifted by one word\n",
        "def prepare_inputs(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    y = tokenized_sentences[:, 1:]\n",
        "    return x, y\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)"
      ],
      "metadata": {
        "id": "sfgAHOdp118j"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Build the LSTM"
      ],
      "metadata": {
        "id": "TZsUQObW2pgu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x = layers.LSTM(128, return_sequences=True)(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "lstm = models.Model(inputs, outputs)\n",
        "lstm.summary()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "MI4ym9622psy",
        "outputId": "65545ba8-ff14-4fdc-9e3d-b435692c34e4"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (\u001b[38;5;33mInputLayer\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │       \u001b[38;5;34m1,000,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m117,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10000\u001b[0m)         │       \u001b[38;5;34m1,290,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,000,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10000</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,407,248\u001b[0m (9.18 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,407,248</span> (9.18 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,407,248\u001b[0m (9.18 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,407,248</span> (9.18 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Train the LSTM"
      ],
      "metadata": {
        "id": "61bA3bh6216j"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = losses.SparseCategoricalCrossentropy()\n",
        "lstm.compile(\"adam\", loss_fn)"
      ],
      "metadata": {
        "id": "WBzbSddk2zNa"
      },
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TextGenerator checkpoint\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index\n",
        "            for index, word in enumerate(index_to_word)\n",
        "        }\n",
        "\n",
        "    def sample_from(self, probs, temperature):\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
        "            y = self.model.predict(np.array([start_tokens]))\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
        "            if 0 <= sample_token < len(self.index_to_word):  # Check if sample_token is within range\n",
        "              start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "              info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "              start_tokens.append(sample_token)\n",
        "            else:\n",
        "              # Handle case where sample_token is out of range\n",
        "              print(f\"Warning: sample_token out of range: {sample_token}\")\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "            info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "            start_tokens.append(sample_token)\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      try:\n",
        "        prompts = ('if this journey', 'it was a dreary')\n",
        "        prompt = np.random.choice(prompts)\n",
        "        self.generate(prompt, max_tokens=100, temperature=.2)\n",
        "      except Exception as e:\n",
        "        print(f\"Error during text generation: {e}\")"
      ],
      "metadata": {
        "id": "YbRUW4D03L9j"
      },
      "execution_count": 30,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize starting prompt\n",
        "\n",
        "text_generator = TextGenerator(vocab)\n"
      ],
      "metadata": {
        "id": "dMBBD8kx2_yr"
      },
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[text_generator],\n",
        ")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ-tfbj23C1z",
        "outputId": "40b7ad4f-e04f-47dd-ce5d-0ef87ebedbcd"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\n",
            "generated text:\n",
            "it was a dreary  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 7.8273e-06\n",
            "Epoch 2/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\n",
            "generated text:\n",
            "if this journey  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 79ms/step - loss: 4.0601e-06\n",
            "Epoch 3/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\n",
            "generated text:\n",
            "if this journey  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 2.2052e-06\n",
            "Epoch 4/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\n",
            "generated text:\n",
            "it was a dreary  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 1.1772e-06\n",
            "Epoch 5/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\n",
            "generated text:\n",
            "it was a dreary  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 6.9003e-07\n",
            "Epoch 6/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\n",
            "generated text:\n",
            "if this journey  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 78ms/step - loss: 3.8996e-07\n",
            "Epoch 7/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\n",
            "generated text:\n",
            "if this journey  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 2.2653e-07\n",
            "Epoch 8/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\n",
            "generated text:\n",
            "it was a dreary  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 1.2877e-07\n",
            "Epoch 9/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "if this journey  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 7.1892e-08\n",
            "Epoch 10/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\n",
            "generated text:\n",
            "it was a dreary  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 4.1164e-08\n",
            "Epoch 11/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "if this journey  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 2.2560e-08\n",
            "Epoch 12/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "it was a dreary  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 78ms/step - loss: 1.2752e-08\n",
            "Epoch 13/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "it was a dreary  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 7.2075e-09\n",
            "Epoch 14/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\n",
            "generated text:\n",
            "if this journey  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m103s\u001b[0m 78ms/step - loss: 4.4313e-09\n",
            "Epoch 15/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\n",
            "generated text:\n",
            "it was a dreary  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 2.4794e-09\n",
            "Epoch 16/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\n",
            "generated text:\n",
            "if this journey  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 1.5057e-09\n",
            "Epoch 17/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "it was a dreary  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 9.8160e-10\n",
            "Epoch 18/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "if this journey  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 6.7489e-10\n",
            "Epoch 19/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "if this journey  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 4.7498e-10\n",
            "Epoch 20/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\n",
            "generated text:\n",
            "it was a dreary  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 3.7199e-10\n",
            "Epoch 21/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\n",
            "generated text:\n",
            "it was a dreary  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 3.0173e-10\n",
            "Epoch 22/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\n",
            "generated text:\n",
            "if this journey  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m104s\u001b[0m 78ms/step - loss: 2.5124e-10\n",
            "Epoch 23/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "if this journey  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m142s\u001b[0m 79ms/step - loss: 2.0504e-10\n",
            "Epoch 24/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\n",
            "generated text:\n",
            "it was a dreary  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 79ms/step - loss: 1.7738e-10\n",
            "Epoch 25/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\n",
            "generated text:\n",
            "it was a dreary  \n",
            "\n",
            "\u001b[1m1330/1330\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m105s\u001b[0m 79ms/step - loss: 1.5574e-10\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7b4dae5957e0>"
            ]
          },
          "metadata": {},
          "execution_count": 32
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Generate Text"
      ],
      "metadata": {
        "id": "go6WtJ5I3mzu"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            if 0 <= i < len(vocab):\n",
        "                print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "            else:\n",
        "                print(f\"Index {i} out of range for vocabulary (size: {len(vocab)})\") # Print error message\n",
        "        print(\"--------\\n\")"
      ],
      "metadata": {
        "id": "ohzdYaVN38bq"
      },
      "execution_count": 33,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"it was a dreary\", max_tokens=10, temperature=.2\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "SfN-DQI13ri6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2906533-e852-45df-c940-d1e2a579a429"
      },
      "execution_count": 34,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\n",
            "generated text:\n",
            "it was a dreary  \n",
            "\n",
            "\n",
            "PROMPT: it was a dreary \n",
            ":   \t100.0%\n",
            "preserved:   \t0.0%\n",
            "prevail:   \t0.0%\n",
            "prevailed:   \t0.0%\n",
            "prevented:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was a dreary  \n",
            ":   \t100.0%\n",
            "preserved:   \t0.0%\n",
            "prevail:   \t0.0%\n",
            "prevailed:   \t0.0%\n",
            "prevented:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"it was a splendid morning\", max_tokens=10, temperature=0.2\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "MnMJHTdu3viT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d4ae2645-1e84-4d99-d8f0-decf3aaab964"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "it was a splendid morning \n",
            "\n",
            "\n",
            "PROMPT: it was a splendid morning \n",
            ":   \t100.0%\n",
            "distinctly:   \t0.0%\n",
            "district:   \t0.0%\n",
            "disturbed:   \t0.0%\n",
            "diversion:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"it was a splendid morning\", max_tokens=30, temperature=0.8)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "hPaf5l2n2ME1",
        "outputId": "f463cb5f-d3c5-4dad-e52c-02c494c33af8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "it was a splendid morning \n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"the meaning of life\", max_tokens=15, temperature=1.0\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "LytOhBMZ32Mo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "270329ca-31c7-425b-8f17-1f8e0fa9ce8c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "the meaning of life \n",
            "\n",
            "\n",
            "PROMPT: the meaning of life \n",
            ":   \t100.0%\n",
            "cordiality:   \t0.0%\n",
            "interrogatively:   \t0.0%\n",
            "Index 9540 out of range for vocabulary (size: 7307)\n",
            "rhythmically:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"the meaning of life\", max_tokens=50, temperature=0.2\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "2Nhcu_8e34Hg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f8a1c155-fbd8-4b5e-ca70-753e0f7a69ab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "generated text:\n",
            "the meaning of life \n",
            "\n",
            "\n",
            "PROMPT: the meaning of life \n",
            ":   \t100.0%\n",
            "distinctly:   \t0.0%\n",
            "district:   \t0.0%\n",
            "disturbed:   \t0.0%\n",
            "diversion:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    }
  ]
}