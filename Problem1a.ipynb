{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/KelseyNager/GenAI/blob/main/Problem1a.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FSrjyb8l1uzT"
      },
      "source": [
        "#LSTM\n",
        "##Kelsey Nager\n",
        "##CSC 330"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#0 Parameters"
      ],
      "metadata": {
        "id": "3nzw9tOeWyWt"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "P2rzFk5-rblV"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import json\n",
        "import re\n",
        "import string\n",
        "\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras import layers, models, callbacks, losses"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 59,
      "metadata": {
        "id": "U3egQ336rwa_"
      },
      "outputs": [],
      "source": [
        "VOCAB_SIZE = 20000\n",
        "MAX_LEN = 150\n",
        "EMBEDDING_DIM = 100\n",
        "VALIDATION_SPLIT = 0.2\n",
        "SEED = 42\n",
        "LOAD_MODEL = False\n",
        "BATCH_SIZE = 64\n",
        "EPOCHS = 25"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJ9DJyoq1wUJ"
      },
      "source": [
        "#1 Data Collection and Preparation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CfqXZdFdp4C7",
        "outputId": "25daa870-fd4f-45da-881f-5782fbfcdc35"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Start match found: True\n",
            "End match found: True\n",
            "Start match found: True\n",
            "End match found: True\n",
            "Start match found: True\n",
            "End match found: True\n"
          ]
        }
      ],
      "source": [
        "import requests\n",
        "import re\n",
        "\n",
        "\n",
        "def trim_book_content(book_content, start, end):\n",
        "    \"\"\"Trims the beginning and end of book content using markers.\"\"\"\n",
        "    start_match = re.search(re.escape(start), book_content)\n",
        "    end_match = re.search(re.escape(end), book_content)\n",
        "\n",
        "    print(f\"Start match found: {start_match is not None}\")  # Check if start marker is found\n",
        "    print(f\"End match found: {end_match is not None}\")    # Check if end marker is found\n",
        "\n",
        "    if start_match and end_match:\n",
        "        start_index = start_match.end()\n",
        "        end_index = end_match.start()\n",
        "        trimmed_content = book_content[start_index:end_index]\n",
        "        return trimmed_content\n",
        "    return \"\"\n",
        "\n",
        "\n",
        "# Download each text file and append to all_books\n",
        "urls = [\n",
        "\"https://www.gutenberg.org/files/71865/71865-0.txt\",  # Mrs Dalloway, Virginia Woolf\n",
        "\"https://www.gutenberg.org/files/144/144-0.txt\",   # The Voyage Out, Virginia Woolf\n",
        "\"https://www.gutenberg.org/files/64457/64457-0.txt\"   # The Common Reader, Virginia Woolf\n",
        "      ]\n",
        "\n",
        "start = \"*** START OF THE PROJECT GUTENBERG EBOOK\"\n",
        "end = \"*** END OF THE PROJECT GUTENBERG EBOOK\"\n",
        "\n",
        "all_books = \"\"\n",
        "\n",
        "# Save combined text to a single file\n",
        "for url in urls:\n",
        "  response = requests.get(url)\n",
        "  book_content = response.text\n",
        "  trimmed_text = trim_book_content(book_content, start, end)\n",
        "  all_books += trimmed_text + \"\\n\\n\"\n",
        "\n",
        "with open('all_books_trimmed.txt', 'w', encoding='utf-8') as file:\n",
        "    file.write(all_books)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with open(\"all_books_trimmed.txt\", \"r\", encoding=\"utf-8\") as file:\n",
        "    all_books = file.read()\n",
        "\n",
        "# Split the text into lines\n",
        "book_data = all_books.split(\"\\n\")\n",
        "\n",
        "#filtered_data represents all three combined, filtered Vrignia Woolf books splint into lines\n",
        "filtered_data = [\n",
        "    \"Text: \" + line\n",
        "    for line in book_data\n",
        "    if line.strip()\n",
        "]"
      ],
      "metadata": {
        "id": "p-qRWzQzPpmX"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 23,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KAvTF1SsK4eY",
        "outputId": "73d62c98-bb2b-4abd-f8a7-07721051869c"
      },
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Text: Elizabeth), and she, too, loving it as she did with an absurd and'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 23
        }
      ],
      "source": [
        "# Display an example word\n",
        "example = filtered_data[100]\n",
        "example"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 24,
      "metadata": {
        "id": "70pfkORxsJYh"
      },
      "outputs": [],
      "source": [
        "def pad_punctuation(s):\n",
        "    s = re.sub(f\"([{string.punctuation}])\", r\" \\1 \", s)  # Pad punctuation\n",
        "    s = re.sub(\" +\", \" \", s)\n",
        "    s = s.lower()  # Convert to lowercase for consistency\n",
        "    return s\n",
        "\n",
        "text_data = [pad_punctuation(s) for s in filtered_data]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 25,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l-UOtajoL6oi",
        "outputId": "a8fb67bb-1569-45ed-ce04-4f7b321bbb69"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Number of lines of text of filtered data: 24761\n"
          ]
        }
      ],
      "source": [
        "print(f\"Number of lines of text of filtered data: {len(filtered_data)}\")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#same example as earlier, now with padded punctuation and lowercase letters\n",
        "example_data = text_data[100]\n",
        "example_data"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "aeJ8tDHvRvWt",
        "outputId": "b8a9e0b7-c1f4-4add-d10e-99b9de1e9aeb"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'text : elizabeth ) , and she , too , loving it as she did with an absurd and'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "id": "mr9FuN79sKT6"
      },
      "outputs": [],
      "source": [
        "# Convert to a Tensorflow Dataset\n",
        "text_ds = (\n",
        "    tf.data.Dataset.from_tensor_slices(text_data)\n",
        "    .batch(BATCH_SIZE)\n",
        "    .shuffle(1000)\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#example of lines\n",
        "for example in text_ds.take(1):\n",
        "       print(example)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qu-F6VYiEcWw",
        "outputId": "8f5ba896-393c-4dcb-fca2-ddf6e1fc211d"
      },
      "execution_count": 28,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "tf.Tensor(\n",
            "[b'text : random chatter about some book just out an authority now entirely to'\n",
            " b'text : seek . the diverse schools would have debated as hotly as ever , but at'\n",
            " b'text : the back of every reader\\xe2\\x80\\x99s mind would have been the consciousness that'\n",
            " b'text : there was at least one man who kept the main principles of literature'\n",
            " b'text : closely in view ; who , if you had taken to him some eccentricity of the'\n",
            " b'text : moment , would have brought it into touch with permanence and tethered it'\n",
            " b'text : by his own authority in the contrary blasts of praise and blame . [ 15 ] but'\n",
            " b'text : when it comes to the making of a critic , nature must be generous and'\n",
            " b'text : society ripe . the scattered dinner - tables of the modern world , the chase'\n",
            " b'text : and eddy of the various currents which compose the society of our time , '\n",
            " b'text : could only be dominated by a giant of fabulous dimensions . and where is'\n",
            " b'text : even the very tall man whom we have the right to expect ? reviewers we'\n",
            " b'text : have but no critic ; a million competent and incorruptible policemen but'\n",
            " b'text : no judge . men of taste and learning and ability are for ever lecturing'\n",
            " b'text : the young and celebrating the dead . but the too frequent result of their'\n",
            " b'text : able and industrious pens is a desiccation of the living tissues of'\n",
            " b'text : literature into a network of little bones . nowhere shall we find the'\n",
            " b'text : downright vigour of a dryden , or keats with his fine and natural'\n",
            " b'text : bearing , his profound insight and sanity , or flaubert and the tremendous'\n",
            " b'text : power of his fanaticism , or coleridge , above all , brewing in his head'\n",
            " b'text : the whole of poetry and letting issue now and then one of those profound'\n",
            " b'text : general statements which are caught up by the mind when hot with the'\n",
            " b'text : friction of reading as if they were of the soul of the book itself . '\n",
            " b'text : and to all this , too , the critics generously agree . a great critic , they'\n",
            " b'text : say , is the rarest of beings . but should one miraculously appear , how'\n",
            " b'text : should we maintain him , on what should we feed him ? great critics , if'\n",
            " b'text : they are not themselves great poets , are bred from the profusion of the'\n",
            " b'text : age . there is some great man to be vindicated , some school to be founded'\n",
            " b'text : or destroyed . but our age is meagre to the verge of destitution . there'\n",
            " b'text : is no name which dominates the rest . there is no master in whose'\n",
            " b'text : workshop the young are proud to serve apprenticeship . mr . hardy has long'\n",
            " b'text : since withdrawn from the arena , and there is something exotic about the'\n",
            " b'text : genius of mr . conrad which makes him not so much an influence as an'\n",
            " b'text : idol , honoured and admired , but aloof and apart . as for the rest , though'\n",
            " b'text : they are many and vigorous and in the full flood of creative activity , '\n",
            " b'text : there is none whose influence can seriously affect his contemporaries , '\n",
            " b'text : or penetrate beyond our day to that not very distant future which it'\n",
            " b'text : pleases us to call immortality . if we make a century our test , and ask'\n",
            " b'text : how much of the work produced in these days in england will be in'\n",
            " b'text : existence then , we shall have to answer not merely that we cannot agree'\n",
            " b'text : upon the same book , but that we are more than doubtful whether such a'\n",
            " b'text : book there is . it is an age of fragments . a few stanzas , a few pages , a'\n",
            " b'text : chapter here and there , the beginning of this novel , the end of that , '\n",
            " b'text : are equal to the best of any age or author . but can we go to posterity'\n",
            " b'text : with a sheaf of loose pages , or ask the readers of those days , with the'\n",
            " b'text : whole of literature before them , to sift our enormous rubbish heaps for'\n",
            " b'text : our tiny pearls ? such are the questions which the critics might lawfully'\n",
            " b'text : put to their companions at table , the novelists and poets . '\n",
            " b'text : at first the weight of pessimism seems sufficient to bear down all'\n",
            " b'text : opposition . yes , it is a lean age , we repeat , with much to justify its'\n",
            " b'text : poverty ; but , frankly , if we pit one century against another the'\n",
            " b'text : comparison seems overwhelmingly against us . _ waverley , the excursion , '\n",
            " b'text : kubla khan , don juan , hazlitt\\xe2\\x80\\x99s essays , pride and prejudice , hyperion _ , '\n",
            " b'text : and _ prometheus unbound _ were all published between 1800 and 1821 . our'\n",
            " b'text : century has not lacked industry ; but if we ask for masterpieces it'\n",
            " b'text : appears on the face of it that the pessimists are right . it seems as if'\n",
            " b'text : an age of genius must be succeeded by an age of endeavour ; riot and'\n",
            " b'text : extravagance by cleanliness and hard work . all honour , of course , to'\n",
            " b'text : those who have sacrificed their immortality to set the house in order . '\n",
            " b'text : but if we ask for masterpieces , where are we to look ? a little poetry , '\n",
            " b'text : we may feel sure , will survive ; a few poems by mr . yeats , by mr . davies , '\n",
            " b'text : by mr . de la mare . mr . lawrence , of course , has moments of greatness , '\n",
            " b'text : but hours of something very different . mr . beerbohm , in his way , is'\n",
            " b'text : perfect , but it is not a big way . passages in _ far away and long ago _ '], shape=(64,), dtype=string)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "id": "ZwRkI5HbsKXb"
      },
      "outputs": [],
      "source": [
        "# Create a vectorization layer\n",
        "vectorize_layer = layers.TextVectorization(\n",
        "    standardize=\"lower\",\n",
        "    max_tokens=VOCAB_SIZE + 1,\n",
        "    output_mode=\"int\",\n",
        "    output_sequence_length=MAX_LEN + 1,\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {
        "id": "7jgxCJ6XsR30",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9efa029a-b06c-43b9-80a4-46bd05c4dd2a"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 18612\n"
          ]
        }
      ],
      "source": [
        "# Adapt the layer to the training set\n",
        "vectorize_layer.adapt(text_ds)\n",
        "vocab = vectorize_layer.get_vocabulary()\n",
        "print(\"Vocabulary size:\", len(vocab))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l1uQdQAa65ga",
        "outputId": "da9edc36-9858-46f3-914c-360ae58584d4"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0: \n",
            "1: [UNK]\n",
            "2: :\n",
            "3: text\n",
            "4: ,\n",
            "5: the\n",
            "6: .\n",
            "7: and\n",
            "8: of\n",
            "9: to\n"
          ]
        }
      ],
      "source": [
        "# Display some token:word mappings\n",
        "for i, word in enumerate(vocab[:10]):\n",
        "    print(f\"{i}: {word}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "sfgAHOdp118j"
      },
      "outputs": [],
      "source": [
        "# Create the training set of book content and the same text shifted by one word\n",
        "def prepare_inputs(text):\n",
        "    text = tf.expand_dims(text, -1)\n",
        "    tokenized_sentences = vectorize_layer(text)\n",
        "    x = tokenized_sentences[:, :-1]\n",
        "    return x, tokenized_sentences[:, 1:]\n",
        "\n",
        "train_ds = text_ds.map(prepare_inputs)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TZsUQObW2pgu"
      },
      "source": [
        "# Single-Layer LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 33,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 257
        },
        "id": "MI4ym9622psy",
        "outputId": "f15cf3c7-d149-44ae-f364-8dd9b5a25f8c"
      },
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_1 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │       \u001b[38;5;34m2,500,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m117,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m25000\u001b[0m)         │       \u001b[38;5;34m3,225,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,500,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">25000</span>)         │       <span style=\"color: #00af00; text-decoration-color: #00af00\">3,225,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m5,842,248\u001b[0m (22.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,842,248</span> (22.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m5,842,248\u001b[0m (22.29 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">5,842,248</span> (22.29 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ],
      "source": [
        "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x = layers.LSTM(128, return_sequences=True, dropout=0.2)(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "lstm_1 = models.Model(inputs, outputs)\n",
        "lstm_1.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "61bA3bh6216j"
      },
      "source": [
        "#Training Single-Layer LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 34,
      "metadata": {
        "id": "WBzbSddk2zNa"
      },
      "outputs": [],
      "source": [
        "loss_fn = losses.SparseCategoricalCrossentropy()\n",
        "lstm_1.compile(\"adam\", loss_fn)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "YbRUW4D03L9j"
      },
      "outputs": [],
      "source": [
        "# Create a TextGenerator checkpoint\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index\n",
        "            for index, word in enumerate(index_to_word)\n",
        "        }\n",
        "\n",
        "    def sample_from(self, probs, temperature):\n",
        "        if isinstance(probs, (float, np.float64)):  # Check if probs is a single value\n",
        "            probs = np.array([probs, 1 - probs])  # Create a 2-element distribution\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len([\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]) < max_tokens and sample_token != 0:\n",
        "            y = self.model.predict(np.array([[\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]]))\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
        "\n",
        "            if 0 <= sample_token < len(self.index_to_word):  # Check if sample_token is within range\n",
        "              start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "              info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "              [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ].append(sample_token)\n",
        "            else:\n",
        "              # Handle case where sample_token is out of range\n",
        "              print(f\"Warning: sample_token out of range: {sample_token}\")\n",
        "              break\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "            info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "            [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ].append(sample_token)\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      try:\n",
        "        prompts = ('the meaning of life', 'it is an awful')\n",
        "        prompt = np.random.choice(prompts)\n",
        "        self.generate(prompt, max_tokens=100, temperature=.5)\n",
        "      except Exception as e:\n",
        "        print(f\"Error during text generation: {e}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "dMBBD8kx2_yr"
      },
      "outputs": [],
      "source": [
        "# Tokenize starting prompt\n",
        "\n",
        "text_generator = TextGenerator(vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "eZ-tfbj23C1z",
        "outputId": "f72536a6-e0fc-48b7-dd5b-fa9d4c2f3f01"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Warning: sample_token out of range: 18667\n",
            "\n",
            "generated text:\n",
            "it is an awful intellectuals intellectuals jingle jingle\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 150ms/step - loss: 0.5625\n",
            "Epoch 2/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life vinraces vinraces  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 150ms/step - loss: 0.5470\n",
            "Epoch 3/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 39ms/step\n",
            "Warning: sample_token out of range: 21866\n",
            "\n",
            "generated text:\n",
            "it is an awful\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 151ms/step - loss: 0.5354\n",
            "Epoch 4/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 23426\n",
            "\n",
            "generated text:\n",
            "it is an awful sniffing sniffing\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 151ms/step - loss: 0.5187\n",
            "Epoch 5/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Warning: sample_token out of range: 20871\n",
            "\n",
            "generated text:\n",
            "it is an awful olney olney\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 151ms/step - loss: 0.5102\n",
            "Epoch 6/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 21154\n",
            "\n",
            "generated text:\n",
            "it is an awful\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 151ms/step - loss: 0.4984\n",
            "Epoch 7/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life stood stood . .  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 152ms/step - loss: 0.4912\n",
            "Epoch 8/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life imbed imbed clutching clutching  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 153ms/step - loss: 0.4842\n",
            "Epoch 9/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Warning: sample_token out of range: 18924\n",
            "\n",
            "generated text:\n",
            "it is an awful unlikely unlikely\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m58s\u001b[0m 151ms/step - loss: 0.4823\n",
            "Epoch 10/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life charles charles was was  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 153ms/step - loss: 0.4745\n",
            "Epoch 11/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Warning: sample_token out of range: 23665\n",
            "\n",
            "generated text:\n",
            "the meaning of life\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 153ms/step - loss: 0.4677\n",
            "Epoch 12/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life preserved preserved and and those those  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 153ms/step - loss: 0.4609\n",
            "Epoch 13/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life spain spain  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 151ms/step - loss: 0.4549\n",
            "Epoch 14/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 20593\n",
            "\n",
            "generated text:\n",
            "it is an awful\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 154ms/step - loss: 0.4529\n",
            "Epoch 15/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 19364\n",
            "\n",
            "generated text:\n",
            "it is an awful\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 152ms/step - loss: 0.4472\n",
            "Epoch 16/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 19283\n",
            "\n",
            "generated text:\n",
            "the meaning of life\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 152ms/step - loss: 0.4406\n",
            "Epoch 17/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "Warning: sample_token out of range: 24584\n",
            "\n",
            "generated text:\n",
            "the meaning of life\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m83s\u001b[0m 154ms/step - loss: 0.4379\n",
            "Epoch 18/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Warning: sample_token out of range: 24495\n",
            "\n",
            "generated text:\n",
            "it is an awful\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 151ms/step - loss: 0.4312\n",
            "Epoch 19/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "Warning: sample_token out of range: 19858\n",
            "\n",
            "generated text:\n",
            "it is an awful sharp sharp euripides euripides territory territory bencher bencher yet—i yet—i sized sized survive survive\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m60s\u001b[0m 154ms/step - loss: 0.4244\n",
            "Epoch 20/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 19235\n",
            "\n",
            "generated text:\n",
            "it is an awful par par affirmed affirmed women—her women—her terence’s terence’s attaching attaching books” books” famished famished ambrose—i ambrose—i\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m82s\u001b[0m 155ms/step - loss: 0.4223\n",
            "Epoch 21/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "Warning: sample_token out of range: 19458\n",
            "\n",
            "generated text:\n",
            "it is an awful angel angel\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 151ms/step - loss: 0.4187\n",
            "Epoch 22/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life pass pass create create . .  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 153ms/step - loss: 0.4121\n",
            "Epoch 23/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Warning: sample_token out of range: 20338\n",
            "\n",
            "generated text:\n",
            "the meaning of life of of worsted worsted fresshe fresshe\n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 152ms/step - loss: 0.4090\n",
            "Epoch 24/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life crocodile crocodile  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 152ms/step - loss: 0.4065\n",
            "Epoch 25/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life scots scots dominoes dominoes ‘quite ‘quite antiquated antiquated  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m59s\u001b[0m 152ms/step - loss: 0.4044\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f0b896717e0>"
            ]
          },
          "metadata": {},
          "execution_count": 41
        }
      ],
      "source": [
        "lstm_1.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[text_generator],\n",
        ")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "go6WtJ5I3mzu"
      },
      "source": [
        "#Text Generation\n",
        "##with Single Layer LSTM"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "id": "ohzdYaVN38bq"
      },
      "outputs": [],
      "source": [
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            if 0 <= i < len(vocab):\n",
        "                print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "            else:\n",
        "                print(f\"Index {i} out of range for vocabulary (size: {len(vocab)})\") # Print error message\n",
        "        print(\"--------\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 1, Various Temperatures"
      ],
      "metadata": {
        "id": "BL6tVa0uUL7d"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "id": "SfN-DQI13ri6",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "236c13f2-5d8c-4d1f-ee35-3bf3840466c3"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 112ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 47ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 85ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life is tretys tretys , , and and\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is tretys\n",
            "laced:   \t0.48%\n",
            "famously:   \t0.46%\n",
            "cakes:   \t0.46%\n",
            "11:   \t0.43%\n",
            "voyage—china:   \t0.42%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is tretys tretys\n",
            "laced:   \t0.48%\n",
            "famously:   \t0.46%\n",
            "cakes:   \t0.46%\n",
            "11:   \t0.43%\n",
            "voyage—china:   \t0.42%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is tretys tretys ,\n",
            ",:   \t85.91%\n",
            ";:   \t11.17%\n",
            ":   \t1.95%\n",
            ".:   \t0.69%\n",
            "?:   \t0.19%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is tretys tretys , ,\n",
            ",:   \t85.91%\n",
            ";:   \t11.17%\n",
            ":   \t1.95%\n",
            ".:   \t0.69%\n",
            "?:   \t0.19%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is tretys tretys , , and\n",
            "and:   \t69.83%\n",
            "which:   \t21.35%\n",
            "”:   \t3.27%\n",
            "or:   \t1.68%\n",
            "but:   \t1.25%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is tretys tretys , , and and\n",
            "and:   \t69.83%\n",
            "which:   \t21.35%\n",
            "”:   \t3.27%\n",
            "or:   \t1.68%\n",
            "but:   \t1.25%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"the meaning of life is\", max_tokens=10, temperature=.2\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "id": "MnMJHTdu3viT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "04965672-ac0a-4b8f-c49f-db54e21a6855"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Warning: sample_token out of range: 23156\n",
            "\n",
            "generated text:\n",
            "the meaning of life is true true rated rated\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is true\n",
            "_:   \t2.75%\n",
            "a:   \t0.74%\n",
            "the:   \t0.52%\n",
            "that:   \t0.44%\n",
            "like:   \t0.4%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is true true\n",
            "_:   \t2.75%\n",
            "a:   \t0.74%\n",
            "the:   \t0.52%\n",
            "that:   \t0.44%\n",
            "like:   \t0.4%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is true true rated\n",
            "footnote:   \t0.66%\n",
            "8:   \t0.35%\n",
            "1:   \t0.32%\n",
            "7:   \t0.31%\n",
            "6:   \t0.3%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is true true rated rated\n",
            "footnote:   \t0.66%\n",
            "8:   \t0.35%\n",
            "1:   \t0.32%\n",
            "7:   \t0.31%\n",
            "6:   \t0.3%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"the meaning of life is\", max_tokens=10, temperature=0.5\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "id": "hPaf5l2n2ME1",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b079704c-f024-4a4c-ca4b-768505b17f18"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire straighten straighten speedily speedily  \n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave\n",
            "_:   \t0.06%\n",
            "captured:   \t0.05%\n",
            "published:   \t0.04%\n",
            "calming:   \t0.04%\n",
            "4:   \t0.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave\n",
            "_:   \t0.06%\n",
            "captured:   \t0.05%\n",
            "published:   \t0.04%\n",
            "calming:   \t0.04%\n",
            "4:   \t0.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable\n",
            "a:   \t13.79%\n",
            "the:   \t10.29%\n",
            "my:   \t5.02%\n",
            "love:   \t4.59%\n",
            "an:   \t3.57%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable\n",
            "a:   \t13.79%\n",
            "the:   \t10.29%\n",
            "my:   \t5.02%\n",
            "love:   \t4.59%\n",
            "an:   \t3.57%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered\n",
            "footnote:   \t0.09%\n",
            "n’avoir:   \t0.05%\n",
            "delineator:   \t0.05%\n",
            "arm’s:   \t0.04%\n",
            "embark:   \t0.04%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered\n",
            "footnote:   \t0.09%\n",
            "n’avoir:   \t0.05%\n",
            "delineator:   \t0.05%\n",
            "arm’s:   \t0.04%\n",
            "embark:   \t0.04%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably\n",
            "footnote:   \t0.29%\n",
            "francis:   \t0.19%\n",
            "undergraduate:   \t0.15%\n",
            "harry:   \t0.12%\n",
            "atheist:   \t0.11%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably\n",
            "footnote:   \t0.29%\n",
            "francis:   \t0.19%\n",
            "undergraduate:   \t0.15%\n",
            "harry:   \t0.12%\n",
            "atheist:   \t0.11%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire\n",
            "morrow:   \t0.1%\n",
            "back—or:   \t0.05%\n",
            "and—do:   \t0.05%\n",
            "sadly:   \t0.05%\n",
            "on—as:   \t0.05%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire\n",
            "morrow:   \t0.1%\n",
            "back—or:   \t0.05%\n",
            "and—do:   \t0.05%\n",
            "sadly:   \t0.05%\n",
            "on—as:   \t0.05%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire straighten\n",
            "morrow:   \t0.18%\n",
            "footnote:   \t0.13%\n",
            "illustration:   \t0.05%\n",
            "breeches:   \t0.04%\n",
            "8:   \t0.04%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire straighten straighten\n",
            "morrow:   \t0.18%\n",
            "footnote:   \t0.13%\n",
            "illustration:   \t0.05%\n",
            "breeches:   \t0.04%\n",
            "8:   \t0.04%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire straighten straighten speedily\n",
            "morrow:   \t0.11%\n",
            "fasten:   \t0.06%\n",
            "euphrosyne:   \t0.06%\n",
            "inferno:   \t0.05%\n",
            "tatler:   \t0.05%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire straighten straighten speedily speedily\n",
            "morrow:   \t0.11%\n",
            "fasten:   \t0.06%\n",
            "euphrosyne:   \t0.06%\n",
            "inferno:   \t0.05%\n",
            "tatler:   \t0.05%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire straighten straighten speedily speedily \n",
            ":   \t99.97%\n",
            ".:   \t0.01%\n",
            ",:   \t0.01%\n",
            "and:   \t0.0%\n",
            "in:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is “leave “leave capable capable surrendered surrendered profitably profitably voltaire voltaire straighten straighten speedily speedily  \n",
            ":   \t99.97%\n",
            ".:   \t0.01%\n",
            ",:   \t0.01%\n",
            "and:   \t0.0%\n",
            "in:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"the meaning of life is\", max_tokens=30, temperature=0.9)\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 2, Various Temperatures"
      ],
      "metadata": {
        "id": "EdGhxq9pUST6"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 58,
      "metadata": {
        "id": "LytOhBMZ32Mo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3d0df63c-84a9-497b-acea-4d193738d0d9"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 21539\n",
            "\n",
            "generated text:\n",
            "it was an awful cowardly cowardly privé” privé” voluminous voluminous ὲν ὲν\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly\n",
            "organ:   \t0.2%\n",
            "submerged:   \t0.15%\n",
            "suffrage:   \t0.13%\n",
            "opulent:   \t0.13%\n",
            "representative:   \t0.12%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly cowardly\n",
            "organ:   \t0.2%\n",
            "submerged:   \t0.15%\n",
            "suffrage:   \t0.13%\n",
            "opulent:   \t0.13%\n",
            "representative:   \t0.12%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly cowardly privé”\n",
            "morrow:   \t0.12%\n",
            "opulent:   \t0.07%\n",
            "allowances:   \t0.06%\n",
            "grossly:   \t0.06%\n",
            "much—everything—in:   \t0.06%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly cowardly privé” privé”\n",
            "morrow:   \t0.12%\n",
            "opulent:   \t0.07%\n",
            "allowances:   \t0.06%\n",
            "grossly:   \t0.06%\n",
            "much—everything—in:   \t0.06%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly cowardly privé” privé” voluminous\n",
            "footnote:   \t1.89%\n",
            "harry:   \t1.62%\n",
            "francis:   \t0.84%\n",
            "morrow:   \t0.58%\n",
            "enchanted:   \t0.51%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly cowardly privé” privé” voluminous voluminous\n",
            "footnote:   \t1.89%\n",
            "harry:   \t1.62%\n",
            "francis:   \t0.84%\n",
            "morrow:   \t0.58%\n",
            "enchanted:   \t0.51%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly cowardly privé” privé” voluminous voluminous ὲν\n",
            "morrow:   \t0.23%\n",
            "footnote:   \t0.14%\n",
            "euphrosyne:   \t0.1%\n",
            "fasten:   \t0.09%\n",
            "maternity:   \t0.09%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful cowardly cowardly privé” privé” voluminous voluminous ὲν ὲν\n",
            "morrow:   \t0.23%\n",
            "footnote:   \t0.14%\n",
            "euphrosyne:   \t0.1%\n",
            "fasten:   \t0.09%\n",
            "maternity:   \t0.09%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    \"it was an awful\", max_tokens=15, temperature=.6\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "id": "2Nhcu_8e34Hg",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d8e7249-6c09-49c4-a94c-c554fcde7e0d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\n",
            "generated text:\n",
            "it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _ . . . . . .\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged\n",
            "deal:   \t9.86%\n",
            "organ:   \t6.91%\n",
            "invalid:   \t2.17%\n",
            "adjustment:   \t1.72%\n",
            "commander:   \t1.71%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged\n",
            "deal:   \t9.86%\n",
            "organ:   \t6.91%\n",
            "invalid:   \t2.17%\n",
            "adjustment:   \t1.72%\n",
            "commander:   \t1.71%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45\n",
            "alluring:   \t0.97%\n",
            "impulsive:   \t0.56%\n",
            "marina:   \t0.52%\n",
            "austen’s:   \t0.5%\n",
            "pocked:   \t0.48%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45\n",
            "alluring:   \t0.97%\n",
            "impulsive:   \t0.56%\n",
            "marina:   \t0.52%\n",
            "austen’s:   \t0.5%\n",
            "pocked:   \t0.48%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his\n",
            "detect:   \t1.34%\n",
            "footnote:   \t1.23%\n",
            "ramble:   \t0.74%\n",
            "select:   \t0.59%\n",
            "edged:   \t0.55%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his\n",
            "detect:   \t1.34%\n",
            "footnote:   \t1.23%\n",
            "ramble:   \t0.74%\n",
            "select:   \t0.59%\n",
            "edged:   \t0.55%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer\n",
            "euphrosyne:   \t9.16%\n",
            "morrow:   \t4.35%\n",
            "religio:   \t3.43%\n",
            "tatler:   \t3.18%\n",
            "care—i:   \t3.18%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer\n",
            "euphrosyne:   \t9.16%\n",
            "morrow:   \t4.35%\n",
            "religio:   \t3.43%\n",
            "tatler:   \t3.18%\n",
            "care—i:   \t3.18%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted\n",
            "footnote:   \t10.53%\n",
            "dieu:   \t1.97%\n",
            "gather:   \t1.39%\n",
            "eyre:   \t1.08%\n",
            "flanks:   \t1.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted\n",
            "footnote:   \t10.53%\n",
            "dieu:   \t1.97%\n",
            "gather:   \t1.39%\n",
            "eyre:   \t1.08%\n",
            "flanks:   \t1.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged\n",
            "dine:   \t8.55%\n",
            "refuse:   \t6.26%\n",
            "detect:   \t3.93%\n",
            "ramble:   \t3.79%\n",
            "lap:   \t3.44%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged\n",
            "dine:   \t8.55%\n",
            "refuse:   \t6.26%\n",
            "detect:   \t3.93%\n",
            "ramble:   \t3.79%\n",
            "lap:   \t3.44%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch\n",
            "ramble:   \t2.17%\n",
            "euphrosyne:   \t1.96%\n",
            "edged:   \t1.91%\n",
            "fasten:   \t1.81%\n",
            "tatler:   \t1.48%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch\n",
            "ramble:   \t2.17%\n",
            "euphrosyne:   \t1.96%\n",
            "edged:   \t1.91%\n",
            "fasten:   \t1.81%\n",
            "tatler:   \t1.48%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after\n",
            "weepest:   \t4.72%\n",
            "footnote:   \t3.51%\n",
            "swee:   \t1.2%\n",
            "shaw:   \t1.03%\n",
            "marbot:   \t0.57%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after\n",
            "weepest:   \t4.72%\n",
            "footnote:   \t3.51%\n",
            "swee:   \t1.2%\n",
            "shaw:   \t1.03%\n",
            "marbot:   \t0.57%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained\n",
            "morrow:   \t8.13%\n",
            "minutes:   \t1.31%\n",
            "exist—“an:   \t0.97%\n",
            "allowances:   \t0.92%\n",
            "present”:   \t0.81%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained\n",
            "morrow:   \t8.13%\n",
            "minutes:   \t1.31%\n",
            "exist—“an:   \t0.97%\n",
            "allowances:   \t0.92%\n",
            "present”:   \t0.81%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield\n",
            "footnote:   \t16.63%\n",
            "detect:   \t1.74%\n",
            "criticise:   \t1.31%\n",
            "stoop:   \t1.2%\n",
            "apologise:   \t1.09%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield\n",
            "footnote:   \t16.63%\n",
            "detect:   \t1.74%\n",
            "criticise:   \t1.31%\n",
            "stoop:   \t1.2%\n",
            "apologise:   \t1.09%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative\n",
            "morrow:   \t8.54%\n",
            "obedience:   \t1.03%\n",
            "breeches:   \t0.88%\n",
            "stilt:   \t0.88%\n",
            "footnote:   \t0.56%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative\n",
            "morrow:   \t8.54%\n",
            "obedience:   \t1.03%\n",
            "breeches:   \t0.88%\n",
            "stilt:   \t0.88%\n",
            "footnote:   \t0.56%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook\n",
            "harry:   \t21.01%\n",
            "footnote:   \t17.86%\n",
            "allan:   \t3.97%\n",
            "francis:   \t2.8%\n",
            "morrow:   \t2.55%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook\n",
            "harry:   \t21.01%\n",
            "footnote:   \t17.86%\n",
            "allan:   \t3.97%\n",
            "francis:   \t2.8%\n",
            "morrow:   \t2.55%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow\n",
            "morrow:   \t39.39%\n",
            "footnote:   \t3.14%\n",
            "hustled:   \t2.53%\n",
            "supposed”:   \t1.19%\n",
            "austen’s:   \t0.74%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow\n",
            "morrow:   \t39.39%\n",
            "footnote:   \t3.14%\n",
            "hustled:   \t2.53%\n",
            "supposed”:   \t1.19%\n",
            "austen’s:   \t0.74%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate\n",
            "footnote:   \t17.55%\n",
            "conquest:   \t7.52%\n",
            "milan:   \t5.66%\n",
            "aspiration:   \t4.21%\n",
            "undergraduate:   \t3.5%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate\n",
            "footnote:   \t17.55%\n",
            "conquest:   \t7.52%\n",
            "milan:   \t5.66%\n",
            "aspiration:   \t4.21%\n",
            "undergraduate:   \t3.5%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate :\n",
            "::   \t99.99%\n",
            "rate:   \t0.01%\n",
            "elements:   \t0.0%\n",
            "extent:   \t0.0%\n",
            "importance:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : :\n",
            "::   \t99.99%\n",
            "rate:   \t0.01%\n",
            "elements:   \t0.0%\n",
            "extent:   \t0.0%\n",
            "importance:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _\n",
            ":   \t39.97%\n",
            "_:   \t13.3%\n",
            "she:   \t5.93%\n",
            "the:   \t5.06%\n",
            "was:   \t4.24%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _\n",
            ":   \t39.97%\n",
            "_:   \t13.3%\n",
            "she:   \t5.93%\n",
            "the:   \t5.06%\n",
            "was:   \t4.24%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a\n",
            "persuasion:   \t33.42%\n",
            "_:   \t19.86%\n",
            "times:   \t12.93%\n",
            "memoirs:   \t11.87%\n",
            "the:   \t6.22%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a\n",
            "persuasion:   \t33.42%\n",
            "_:   \t19.86%\n",
            "times:   \t12.93%\n",
            "memoirs:   \t11.87%\n",
            "the:   \t6.22%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china\n",
            "trestles:   \t2.76%\n",
            "voyage—china:   \t2.54%\n",
            "madrid:   \t1.04%\n",
            "charities:   \t1.02%\n",
            "rotherhithe:   \t0.9%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china\n",
            "trestles:   \t2.76%\n",
            "voyage—china:   \t2.54%\n",
            "madrid:   \t1.04%\n",
            "charities:   \t1.02%\n",
            "rotherhithe:   \t0.9%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici\n",
            "medici:   \t47.83%\n",
            "tales:   \t35.81%\n",
            "_:   \t12.18%\n",
            "review:   \t1.75%\n",
            "house:   \t1.23%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici\n",
            "medici:   \t47.83%\n",
            "tales:   \t35.81%\n",
            "_:   \t12.18%\n",
            "review:   \t1.75%\n",
            "house:   \t1.23%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _\n",
            "_:   \t85.85%\n",
            "the:   \t7.59%\n",
            ".:   \t5.86%\n",
            ",:   \t0.55%\n",
            "that:   \t0.07%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _\n",
            "_:   \t85.85%\n",
            "the:   \t7.59%\n",
            ".:   \t5.86%\n",
            ",:   \t0.55%\n",
            "that:   \t0.07%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _ .\n",
            ".:   \t86.54%\n",
            "?:   \t8.56%\n",
            ",:   \t4.88%\n",
            "_:   \t0.0%\n",
            ";:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _ . .\n",
            ".:   \t86.54%\n",
            "?:   \t8.56%\n",
            ",:   \t4.88%\n",
            "_:   \t0.0%\n",
            ";:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _ . . .\n",
            ".:   \t100.0%\n",
            ":   \t0.0%\n",
            "”:   \t0.0%\n",
            "i:   \t0.0%\n",
            "but:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _ . . . .\n",
            ".:   \t100.0%\n",
            ":   \t0.0%\n",
            "”:   \t0.0%\n",
            "i:   \t0.0%\n",
            "but:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _ . . . . .\n",
            ".:   \t98.69%\n",
            ":   \t1.3%\n",
            "”:   \t0.01%\n",
            "i:   \t0.0%\n",
            "he:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful submerged submerged 1643–45 1643–45 appearance—his appearance—his primer primer squatted squatted edged edged punch punch to—“after to—“after ingrained ingrained hatfield hatfield unimaginative unimaginative overlook overlook morrow morrow undergraduate undergraduate : : _ _ a a voyage—china voyage—china medici medici _ _ . . . . . .\n",
            ".:   \t98.69%\n",
            ":   \t1.3%\n",
            "”:   \t0.01%\n",
            "i:   \t0.0%\n",
            "he:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ],
      "source": [
        "info = text_generator.generate(\n",
        "    \"it was an awful\", max_tokens=50, temperature=0.3\n",
        ")\n",
        "print_probs(info, vocab)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    \"it was an awful\", max_tokens=15, temperature=0.1\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "d7MD1HVX6cPD",
        "outputId": "2cddb417-2cd4-4a7c-ba3f-cf83d582c320"
      },
      "execution_count": 57,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 16ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\n",
            "generated text:\n",
            "it was an awful opulent opulent tatler tatler footnote footnote register register austen’s austen’s footnote footnote\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent\n",
            "opulent:   \t99.17%\n",
            "elaborate:   \t0.2%\n",
            "iceblock:   \t0.11%\n",
            "submerged:   \t0.1%\n",
            "communs:   \t0.06%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent\n",
            "opulent:   \t99.17%\n",
            "elaborate:   \t0.2%\n",
            "iceblock:   \t0.11%\n",
            "submerged:   \t0.1%\n",
            "communs:   \t0.06%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler\n",
            "tatler:   \t63.69%\n",
            "euphrosyne:   \t14.78%\n",
            "odyssey:   \t6.82%\n",
            "religio:   \t6.82%\n",
            "inferno:   \t3.21%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler\n",
            "tatler:   \t63.69%\n",
            "euphrosyne:   \t14.78%\n",
            "odyssey:   \t6.82%\n",
            "religio:   \t6.82%\n",
            "inferno:   \t3.21%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote\n",
            "swee:   \t93.65%\n",
            "footnote:   \t6.12%\n",
            "weepest:   \t0.06%\n",
            "illustration:   \t0.04%\n",
            "hustled:   \t0.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote footnote\n",
            "swee:   \t93.65%\n",
            "footnote:   \t6.12%\n",
            "weepest:   \t0.06%\n",
            "illustration:   \t0.04%\n",
            "hustled:   \t0.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote footnote register\n",
            "trestles:   \t20.72%\n",
            "present”:   \t18.54%\n",
            "register:   \t11.93%\n",
            "had—it:   \t11.35%\n",
            "bees”:   \t1.82%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote footnote register register\n",
            "trestles:   \t20.72%\n",
            "present”:   \t18.54%\n",
            "register:   \t11.93%\n",
            "had—it:   \t11.35%\n",
            "bees”:   \t1.82%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote footnote register register austen’s\n",
            "austen’s:   \t76.87%\n",
            "eyre:   \t22.3%\n",
            "footnote:   \t0.42%\n",
            "colleagues:   \t0.32%\n",
            "flanks:   \t0.02%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote footnote register register austen’s austen’s\n",
            "austen’s:   \t76.87%\n",
            "eyre:   \t22.3%\n",
            "footnote:   \t0.42%\n",
            "colleagues:   \t0.32%\n",
            "flanks:   \t0.02%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote footnote register register austen’s austen’s footnote\n",
            "footnote:   \t55.89%\n",
            "morrow:   \t43.54%\n",
            "swee:   \t0.07%\n",
            "8:   \t0.06%\n",
            "7:   \t0.05%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful opulent opulent tatler tatler footnote footnote register register austen’s austen’s footnote footnote\n",
            "footnote:   \t55.89%\n",
            "morrow:   \t43.54%\n",
            "swee:   \t0.07%\n",
            "8:   \t0.06%\n",
            "7:   \t0.05%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation of Text Generation with Single LSTM"
      ],
      "metadata": {
        "id": "b7UnnP-efyQ-"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Multi-Layer LSTM\n"
      ],
      "metadata": {
        "id": "ZaaRYzz3Thpz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "inputs = layers.Input(shape=(None,), dtype=\"int32\")\n",
        "x = layers.Embedding(VOCAB_SIZE, EMBEDDING_DIM)(inputs)\n",
        "x = layers.LSTM(128, return_sequences=True, dropout=0.3)(x)\n",
        "x = layers.LSTM(64, return_sequences=True, dropout=0.3)(x)\n",
        "x = layers.LSTM(32, return_sequences=True, dropout=0.3)(x)\n",
        "outputs = layers.Dense(VOCAB_SIZE, activation=\"softmax\")(x)\n",
        "lstm_2 = models.Model(inputs, outputs)\n",
        "lstm_2.summary()"
      ],
      "metadata": {
        "id": "i4CD81G4Tk1X",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 321
        },
        "outputId": "708e3bfd-e3f4-4305-89d9-8ce8106cec66"
      },
      "execution_count": 60,
      "outputs": [
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (\u001b[38;5;33mInputLayer\u001b[0m)           │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m)                │               \u001b[38;5;34m0\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_2 (\u001b[38;5;33mEmbedding\u001b[0m)              │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m100\u001b[0m)           │       \u001b[38;5;34m2,000,000\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)           │         \u001b[38;5;34m117,248\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m64\u001b[0m)            │          \u001b[38;5;34m49,408\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (\u001b[38;5;33mLSTM\u001b[0m)                        │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m32\u001b[0m)            │          \u001b[38;5;34m12,416\u001b[0m │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (\u001b[38;5;33mDense\u001b[0m)                      │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m20000\u001b[0m)         │         \u001b[38;5;34m660,000\u001b[0m │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n",
              "│ input_layer_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)           │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>)                │               <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ embedding_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)              │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">100</span>)           │       <span style=\"color: #00af00; text-decoration-color: #00af00\">2,000,000</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)           │         <span style=\"color: #00af00; text-decoration-color: #00af00\">117,248</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">49,408</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ lstm_4 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                        │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)            │          <span style=\"color: #00af00; text-decoration-color: #00af00\">12,416</span> │\n",
              "├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n",
              "│ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                      │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">20000</span>)         │         <span style=\"color: #00af00; text-decoration-color: #00af00\">660,000</span> │\n",
              "└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m2,839,072\u001b[0m (10.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,839,072</span> (10.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m2,839,072\u001b[0m (10.83 MB)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,839,072</span> (10.83 MB)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "display_data",
          "data": {
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
            ],
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
              "</pre>\n"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Training Multi-Layer LSTM"
      ],
      "metadata": {
        "id": "iNCvafOAUZh6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "loss_fn = losses.SparseCategoricalCrossentropy()\n",
        "lstm_2.compile(\"adam\", loss_fn)"
      ],
      "metadata": {
        "id": "S_7oXItCTlCl"
      },
      "execution_count": 61,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a TextGenerator checkpoint\n",
        "class TextGenerator(callbacks.Callback):\n",
        "    def __init__(self, index_to_word, top_k=10):\n",
        "        self.index_to_word = index_to_word\n",
        "        self.word_to_index = {\n",
        "            word: index\n",
        "            for index, word in enumerate(index_to_word)\n",
        "        }\n",
        "\n",
        "    def sample_from(self, probs, temperature):\n",
        "        if isinstance(probs, (float, np.float64)):  # Check if probs is a single value\n",
        "            probs = np.array([probs, 1 - probs])  # Create a 2-element distribution\n",
        "        probs = probs ** (1 / temperature)\n",
        "        probs = probs / np.sum(probs)\n",
        "        return np.random.choice(len(probs), p=probs), probs\n",
        "\n",
        "\n",
        "    def generate(self, start_prompt, max_tokens, temperature):\n",
        "        start_tokens = [\n",
        "            self.word_to_index.get(x, 1) for x in start_prompt.split()\n",
        "        ]\n",
        "        sample_token = None\n",
        "        info = []\n",
        "        while len(start_tokens) < max_tokens and sample_token != 0:\n",
        "            y = self.model.predict(np.array([start_tokens]))\n",
        "            sample_token, probs = self.sample_from(y[0][-1], temperature)\n",
        "            if 0 <= sample_token < len(self.index_to_word):  # Check if sample_token is within range\n",
        "              start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "              info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "              start_tokens.append(sample_token)\n",
        "            else:\n",
        "              # Handle case where sample_token is out of range\n",
        "              print(f\"Warning: sample_token out of range: {sample_token}\")\n",
        "            start_prompt = start_prompt + \" \" + self.index_to_word[sample_token]\n",
        "            info.append({\"prompt\": start_prompt, \"word_probs\": probs})\n",
        "            start_tokens.append(sample_token)\n",
        "        print(f\"\\ngenerated text:\\n{start_prompt}\\n\")\n",
        "        return info\n",
        "\n",
        "    def on_epoch_end(self, epoch, logs=None):\n",
        "      try:\n",
        "        prompts = ('the meaning of life', 'it is an awful')\n",
        "        prompt = np.random.choice(prompts)\n",
        "        self.generate(prompt, max_tokens=100, temperature=.5)\n",
        "      except Exception as e:\n",
        "        print(f\"Error during text generation: {e}\")"
      ],
      "metadata": {
        "id": "hiIeZ8nDTlGH"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Tokenize starting prompt\n",
        "\n",
        "text_generator = TextGenerator(vocab)"
      ],
      "metadata": {
        "id": "ejwg7705Uhvb"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "lstm_2.fit(\n",
        "    train_ds,\n",
        "    epochs=EPOCHS,\n",
        "    callbacks=[text_generator],\n",
        ")"
      ],
      "metadata": {
        "id": "wsd1T6mKUh5I",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4d3e10ee-9ca3-4b46-c597-c59e993563ed"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 457ms/step\n",
            "\n",
            "generated text:\n",
            "it is an awful  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 98ms/step - loss: 5.1032\n",
            "Epoch 2/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 97ms/step - loss: 0.7267\n",
            "Epoch 3/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 327ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Warning: sample_token out of range: 19055\n",
            "Error during text generation: list index out of range\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 101ms/step - loss: 0.6233\n",
            "Epoch 4/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Warning: sample_token out of range: 18892\n",
            "Error during text generation: list index out of range\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 100ms/step - loss: 0.6062\n",
            "Epoch 5/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "Warning: sample_token out of range: 19526\n",
            "Error during text generation: list index out of range\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 100ms/step - loss: 0.5972\n",
            "Epoch 6/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "Warning: sample_token out of range: 19050\n",
            "Error during text generation: list index out of range\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 97ms/step - loss: 0.5824\n",
            "Epoch 7/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life clay clay concurred concurred yellows yellows others—though others—though fluently fluently finality finality buddha buddha , , the the the the . .  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 98ms/step - loss: 0.5686\n",
            "Epoch 8/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\n",
            "generated text:\n",
            "it is an awful remains remains shifting shifting cook’s cook’s tom tom juicy juicy statelily statelily bitten bitten congenial congenial : : and and the the eyes eyes , , but but the the middle middle , , the the  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 100ms/step - loss: 0.5644\n",
            "Epoch 9/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 41ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 38ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life sparrows sparrows war—in war—in groans groans women—no women—no grazed grazed surprised surprised cowards cowards hansom hansom latest latest that that never never , ,  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 100ms/step - loss: 0.5561\n",
            "Epoch 10/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "Warning: sample_token out of range: 19118\n",
            "Error during text generation: list index out of range\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 99ms/step - loss: 0.5530\n",
            "Epoch 11/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life heroic heroic : : the the english english like like the the . . said said , , ” ”  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 99ms/step - loss: 0.5479\n",
            "Epoch 12/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "it is an awful method method antagonist antagonist completion completion cummings cummings . .  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 98ms/step - loss: 0.5442\n",
            "Epoch 13/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life jealousies jealousies : : the the of of the the of of the the - - - - . .  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 99ms/step - loss: 0.5360\n",
            "Epoch 14/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Warning: sample_token out of range: 18849\n",
            "Error during text generation: list index out of range\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 97ms/step - loss: 0.5340\n",
            "Epoch 15/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\n",
            "generated text:\n",
            "it is an awful world—” world—” shepperton shepperton commoner commoner haymakers haymakers fairies fairies gasping gasping swinburne—beowulf swinburne—beowulf of of and and  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 99ms/step - loss: 0.5310\n",
            "Epoch 16/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life handful handful : : in in the the tumbler tumbler in in the the time time and and the the  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 99ms/step - loss: 0.5250\n",
            "Epoch 17/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life peculiar peculiar : : , , as as the the , , and and the the little little allan allan , ,  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 100ms/step - loss: 0.5227\n",
            "Epoch 18/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 23ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 27ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\n",
            "generated text:\n",
            "it is an awful conform conform γίγνεται γίγνεται billowy billowy concord concord aptly aptly crushed crushed butterfly” butterfly” angel angel breadth breadth leather leather practitioners practitioners the the limited limited the the whole whole  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m41s\u001b[0m 100ms/step - loss: 0.5196\n",
            "Epoch 19/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 26ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life deception deception sealing sealing up up the the - - , , and and the the meanbred meanbred , , the the  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 99ms/step - loss: 0.5150\n",
            "Epoch 20/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\n",
            "generated text:\n",
            "it is an awful packs packs soul—“l’âme soul—“l’âme lie lie interval interval museums museums she she were were  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 99ms/step - loss: 0.5128\n",
            "Epoch 21/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 24ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\n",
            "generated text:\n",
            "it is an awful “pale “pale took took legions legions run run brooch brooch colossal colossal promising promising fixes fixes remains remains impervious impervious jumbled jumbled scribbled scribbled farms farms tact tact chuckled chuckled conspiracy conspiracy . . mouthful mouthful understood understood , , the the tone tone  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 102ms/step - loss: 0.5105\n",
            "Epoch 22/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "Warning: sample_token out of range: 19034\n",
            "Error during text generation: list index out of range\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 98ms/step - loss: 0.5085\n",
            "Epoch 23/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 22ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "Warning: sample_token out of range: 19921\n",
            "Error during text generation: list index out of range\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m38s\u001b[0m 98ms/step - loss: 0.5039\n",
            "Epoch 24/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 17ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 25ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\n",
            "generated text:\n",
            "it is an awful cooped cooped “often “often vermilion vermilion stuff”—or stuff”—or : : doctor doctor . . . . she she had had not not her her . . . . she she was was  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 101ms/step - loss: 0.4968\n",
            "Epoch 25/25\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 28ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 29ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 32ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 35ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 37ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 33ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 31ms/step\n",
            "\n",
            "generated text:\n",
            "it is an awful bodily bodily jointed jointed : : was was to to the the world world , , the the world world . . , , ” ”  \n",
            "\n",
            "\u001b[1m387/387\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m39s\u001b[0m 101ms/step - loss: 0.4978\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<keras.src.callbacks.history.History at 0x7f0b8935d690>"
            ]
          },
          "metadata": {},
          "execution_count": 64
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Text Generation\n",
        "##with Multi-Layer LSTM"
      ],
      "metadata": {
        "id": "m6fUxoT3UqRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def print_probs(info, vocab, top_k=5):\n",
        "    for i in info:\n",
        "        print(f\"\\nPROMPT: {i['prompt']}\")\n",
        "        word_probs = i[\"word_probs\"]\n",
        "        p_sorted = np.sort(word_probs)[::-1][:top_k]\n",
        "        i_sorted = np.argsort(word_probs)[::-1][:top_k]\n",
        "        for p, i in zip(p_sorted, i_sorted):\n",
        "            if 0 <= i < len(vocab):\n",
        "                print(f\"{vocab[i]}:   \\t{np.round(100*p,2)}%\")\n",
        "            else:\n",
        "                print(f\"Index {i} out of range for vocabulary (size: {len(vocab)})\") # Print error message\n",
        "        print(\"--------\\n\")"
      ],
      "metadata": {
        "id": "hxdF360LUpjO"
      },
      "execution_count": 65,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 1 with Various Temperatures"
      ],
      "metadata": {
        "id": "NtsUu6ZqVDIM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"the meaning of life is\", max_tokens=10, temperature=.1\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "flkRxvfTVF3S",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f338b657-7e19-4dac-f4ce-0021c98e6ed8"
      },
      "execution_count": 73,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life is delightful delightful prove prove ways ways\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is delightful\n",
            "been:   \t0.77%\n",
            "very:   \t0.17%\n",
            "little:   \t0.11%\n",
            "seen:   \t0.11%\n",
            "footnote:   \t0.08%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is delightful delightful\n",
            "been:   \t0.77%\n",
            "very:   \t0.17%\n",
            "little:   \t0.11%\n",
            "seen:   \t0.11%\n",
            "footnote:   \t0.08%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is delightful delightful prove\n",
            "footnote:   \t0.17%\n",
            "been:   \t0.16%\n",
            "little:   \t0.06%\n",
            "seen:   \t0.06%\n",
            "very:   \t0.06%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is delightful delightful prove prove\n",
            "footnote:   \t0.17%\n",
            "been:   \t0.16%\n",
            "little:   \t0.06%\n",
            "seen:   \t0.06%\n",
            "very:   \t0.06%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is delightful delightful prove prove ways\n",
            "very:   \t0.96%\n",
            "little:   \t0.72%\n",
            "been:   \t0.44%\n",
            "::   \t0.38%\n",
            "man:   \t0.38%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life is delightful delightful prove prove ways ways\n",
            "very:   \t0.96%\n",
            "little:   \t0.72%\n",
            "been:   \t0.44%\n",
            "::   \t0.38%\n",
            "man:   \t0.38%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"the meaning of life\", max_tokens=10, temperature=.5\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "9HcfXkLvVGvo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9c877cb7-bcb5-4d94-deb7-4e5ab869ec78"
      },
      "execution_count": 67,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life dramatic dramatic : : there there\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life dramatic\n",
            "been:   \t0.01%\n",
            "footnote:   \t0.01%\n",
            "seen:   \t0.01%\n",
            "heard:   \t0.01%\n",
            "thought:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life dramatic dramatic\n",
            "been:   \t0.01%\n",
            "footnote:   \t0.01%\n",
            "seen:   \t0.01%\n",
            "heard:   \t0.01%\n",
            "thought:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life dramatic dramatic :\n",
            "::   \t99.98%\n",
            "of:   \t0.0%\n",
            "but:   \t0.0%\n",
            "to:   \t0.0%\n",
            "and:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life dramatic dramatic : :\n",
            "::   \t99.98%\n",
            "of:   \t0.0%\n",
            "but:   \t0.0%\n",
            "to:   \t0.0%\n",
            "and:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life dramatic dramatic : : there\n",
            "the:   \t34.73%\n",
            "and:   \t9.31%\n",
            "of:   \t6.34%\n",
            "she:   \t4.43%\n",
            "to:   \t4.05%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life dramatic dramatic : : there there\n",
            "the:   \t34.73%\n",
            "and:   \t9.31%\n",
            "of:   \t6.34%\n",
            "she:   \t4.43%\n",
            "to:   \t4.05%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"the meaning of life\", max_tokens=10, temperature=.9\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "0OkCuC4JVG0-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c57fa875-c6e0-4b1f-add1-1c06e5c8dff0"
      },
      "execution_count": 69,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life prognosticated prognosticated : : tell tell\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life prognosticated\n",
            "been:   \t0.01%\n",
            "little:   \t0.01%\n",
            "very:   \t0.01%\n",
            "footnote:   \t0.01%\n",
            "great:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life prognosticated prognosticated\n",
            "been:   \t0.01%\n",
            "little:   \t0.01%\n",
            "very:   \t0.01%\n",
            "footnote:   \t0.01%\n",
            "great:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life prognosticated prognosticated :\n",
            "::   \t99.99%\n",
            "but:   \t0.0%\n",
            "“i:   \t0.0%\n",
            "and:   \t0.0%\n",
            "“and:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life prognosticated prognosticated : :\n",
            "::   \t99.99%\n",
            "but:   \t0.0%\n",
            "“i:   \t0.0%\n",
            "and:   \t0.0%\n",
            "“and:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life prognosticated prognosticated : : tell\n",
            "the:   \t13.39%\n",
            "she:   \t7.86%\n",
            "he:   \t5.55%\n",
            "they:   \t2.9%\n",
            "it:   \t2.86%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life prognosticated prognosticated : : tell tell\n",
            "the:   \t13.39%\n",
            "she:   \t7.86%\n",
            "he:   \t5.55%\n",
            "they:   \t2.9%\n",
            "it:   \t2.86%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Prompt 2 with Various Temperatures"
      ],
      "metadata": {
        "id": "YydmisQzVILV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"the meaning of life\", max_tokens=10, temperature=.1\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "jYM44EVrVWaF",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "09798788-c426-406b-c8c2-cd952fb46ffb"
      },
      "execution_count": 74,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 21ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life scarcely scarcely olenin olenin : :\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life scarcely\n",
            "little:   \t0.36%\n",
            "been:   \t0.32%\n",
            "very:   \t0.3%\n",
            "great:   \t0.2%\n",
            "good:   \t0.15%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life scarcely scarcely\n",
            "little:   \t0.36%\n",
            "been:   \t0.32%\n",
            "very:   \t0.3%\n",
            "great:   \t0.2%\n",
            "good:   \t0.15%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life scarcely scarcely olenin\n",
            "::   \t1.56%\n",
            "little:   \t0.54%\n",
            "very:   \t0.4%\n",
            "great:   \t0.27%\n",
            "good:   \t0.19%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life scarcely scarcely olenin olenin\n",
            "::   \t1.56%\n",
            "little:   \t0.54%\n",
            "very:   \t0.4%\n",
            "great:   \t0.27%\n",
            "good:   \t0.19%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life scarcely scarcely olenin olenin :\n",
            "::   \t100.0%\n",
            "“i:   \t0.0%\n",
            "but:   \t0.0%\n",
            "“and:   \t0.0%\n",
            "“but:   \t0.0%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life scarcely scarcely olenin olenin : :\n",
            "::   \t100.0%\n",
            "“i:   \t0.0%\n",
            "but:   \t0.0%\n",
            "“and:   \t0.0%\n",
            "“but:   \t0.0%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"the meaning of life\", max_tokens=10, temperature=.5\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "tVduzLgXVOZ2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c97b2fea-77e9-4309-dcff-7b1503b4a7f2"
      },
      "execution_count": 75,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 30ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 34ms/step\n",
            "\n",
            "generated text:\n",
            "the meaning of life nieces nieces authors’ authors’ walker walker\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life nieces\n",
            "footnote:   \t0.01%\n",
            "been:   \t0.01%\n",
            "little:   \t0.01%\n",
            "ascent:   \t0.01%\n",
            "telegram:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life nieces nieces\n",
            "footnote:   \t0.01%\n",
            "been:   \t0.01%\n",
            "little:   \t0.01%\n",
            "ascent:   \t0.01%\n",
            "telegram:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life nieces nieces authors’\n",
            "very:   \t0.02%\n",
            "little:   \t0.01%\n",
            "great:   \t0.01%\n",
            "good:   \t0.01%\n",
            "more:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life nieces nieces authors’ authors’\n",
            "very:   \t0.02%\n",
            "little:   \t0.01%\n",
            "great:   \t0.01%\n",
            "good:   \t0.01%\n",
            "more:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life nieces nieces authors’ authors’ walker\n",
            "::   \t0.73%\n",
            "of:   \t0.04%\n",
            "into:   \t0.04%\n",
            "who:   \t0.03%\n",
            "between:   \t0.03%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: the meaning of life nieces nieces authors’ authors’ walker walker\n",
            "::   \t0.73%\n",
            "of:   \t0.04%\n",
            "into:   \t0.04%\n",
            "who:   \t0.03%\n",
            "between:   \t0.03%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "info = text_generator.generate(\n",
        "    start_prompt=\"it was an awful\", max_tokens=10, temperature=.8\n",
        ")\n",
        "print_probs(info, vocab)"
      ],
      "metadata": {
        "id": "3-ONTd2dVNsL",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4c646140-9df1-40e9-aec2-2730374f9f1c"
      },
      "execution_count": 76,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 20ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 19ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 18ms/step\n",
            "\n",
            "generated text:\n",
            "it was an awful ties ties terms terms trade trade\n",
            "\n",
            "\n",
            "PROMPT: it was an awful ties\n",
            "footnote:   \t0.01%\n",
            "standstill:   \t0.01%\n",
            "main:   \t0.01%\n",
            "goal:   \t0.01%\n",
            "wives:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful ties ties\n",
            "footnote:   \t0.01%\n",
            "standstill:   \t0.01%\n",
            "main:   \t0.01%\n",
            "goal:   \t0.01%\n",
            "wives:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful ties ties terms\n",
            "::   \t0.01%\n",
            "little:   \t0.01%\n",
            "very:   \t0.01%\n",
            "great:   \t0.01%\n",
            "man:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful ties ties terms terms\n",
            "::   \t0.01%\n",
            "little:   \t0.01%\n",
            "very:   \t0.01%\n",
            "great:   \t0.01%\n",
            "man:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful ties ties terms terms trade\n",
            "footnote:   \t0.01%\n",
            "been:   \t0.01%\n",
            "wives:   \t0.01%\n",
            "very:   \t0.01%\n",
            "little:   \t0.01%\n",
            "--------\n",
            "\n",
            "\n",
            "PROMPT: it was an awful ties ties terms terms trade trade\n",
            "footnote:   \t0.01%\n",
            "been:   \t0.01%\n",
            "wives:   \t0.01%\n",
            "very:   \t0.01%\n",
            "little:   \t0.01%\n",
            "--------\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#Evaluation of Text Generation with Multi-Layer LSTM"
      ],
      "metadata": {
        "id": "avttg4agf2Xd"
      }
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}